{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training-gpt2-language-model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPU2AawCcsDs38nI/No/ON6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/6-text-generation-with-gpt-2-and-gpt-3-models/training_gpt2_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TihxpsDJR1fq"
      },
      "source": [
        "## Training a GPT-2 language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nseBpmVGSBw7"
      },
      "source": [
        "This notebook will train a GPT-2 model on a custom dataset that we will encode. We will then interact with our customized model. We will be using the kant.txt dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLgBlVPJN2N-"
      },
      "source": [
        "## Prerequisites and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HcC1b_zN4Pw"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget https://raw.githubusercontent.com/nshepperd/gpt-2/finetuning/train.py\n",
        "wget https://raw.githubusercontent.com/nshepperd/gpt-2/finetuning/src/load_dataset.py\n",
        "wget https://raw.githubusercontent.com/nshepperd/gpt-2/finetuning/encode.py\n",
        "wget https://raw.githubusercontent.com/nshepperd/gpt-2/finetuning/src/accumulate.py\n",
        "wget https://raw.githubusercontent.com/nshepperd/gpt-2/finetuning/src/memory_saving_gradients.py\n",
        "\n",
        "wget https://github.com/PacktPublishing/Transformers-for-Natural-Language-Processing/raw/main/Chapter06/gpt-2-train_files/dset.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5JYf_1VUypz"
      },
      "source": [
        "## Step 1: Initial steps of the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1KSjKpDUzxY"
      },
      "source": [
        "The program now clones OpenAI's GPT-2 repository and not N Shepperd's repository:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHakKl1rVP2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33956097-0c74-4b2f-c0e9-0b72db3424a6"
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 11.58 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbG7vOrTVzA2"
      },
      "source": [
        "We have already uploaded the files we need to train the GPT-2 model from N\n",
        "Shepperd's directory.\n",
        "\n",
        "The program now installs the requirements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJHze0spQ5ZW"
      },
      "source": [
        "import os #when the VM restarts import os necessary\n",
        "\n",
        "os.chdir(\"/content/gpt-2\")\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAoBLtCuRFWt"
      },
      "source": [
        "This notebook requires toposort, which is a topological sort algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbPjwYGaRF2o"
      },
      "source": [
        "!pip install toposort"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgHr2gbDRRJg"
      },
      "source": [
        ">Do not restart the notebook after installing the requirements. Wait until you have checked the TensorFlow version to restart the VM only once during your session. Then restart it if necessary.\n",
        "\n",
        "We now check the TensorFlow version to make sure we are running version tf 1.x:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuPFNxvdRW8o",
        "outputId": "c477471d-b6f5-4c1b-fe88-6a544ec90773"
      },
      "source": [
        "#Colab has tf 1.x , and tf 2.x installed\n",
        "#Restart runtime using 'Runtime' -> 'Restart runtime...'\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpHRFh-uRoHL"
      },
      "source": [
        "The program now downloads the 117M parameter GPT-2 model we will train with\n",
        "our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gtzL8fHRoux",
        "outputId": "1069ca8d-efd2-49b4-ccbe-a05e4a2386ec"
      },
      "source": [
        "import os # after runtime is restarted\n",
        "\n",
        "os.chdir(\"/content/gpt-2\")\n",
        "!python3 download_model.py '117M' #creates model directory"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 1.13Mit/s]                                                     \n",
            "Fetching encoder.json: 1.04Mit [00:00, 3.42Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.05Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:35, 14.0Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.14Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 1.77Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.69Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAYGeMqzRyGj"
      },
      "source": [
        "We will copy the dataset and the 117M parameter GPT-2 model into the src\n",
        "directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFl_e21yRyrA"
      },
      "source": [
        "# Copying the Project Resources to src\n",
        "!cp /content/dset.txt /content/gpt-2/src/\n",
        "!cp -r /content/gpt-2/models/ /content/gpt-2/src/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8OWL6GQSAJf"
      },
      "source": [
        "The goal is to group all of the resources we need to train the model in the src project directory.\n",
        "\n",
        "We will now go through the N Shepperd training files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFhi6Xn-V4cn"
      },
      "source": [
        "## Step 2: The N Shepperd training files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayHpIY5YV6N8"
      },
      "source": [
        "The training files we will use come from N Shepperd's GitHub repository. We\n",
        "already uploaded them. We will now copy them into our project directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdE7wTOtXf5c"
      },
      "source": [
        "!cp /content/train.py /content/gpt-2/src/\n",
        "!cp /content/load_dataset.py /content/gpt-2/src/\n",
        "!cp /content/encode.py /content/gpt-2/src/\n",
        "!cp /content/accumulate.py /content/gpt-2/src/\n",
        "!cp /content/memory_saving_gradients.py /content/gpt-2/src/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6Mp3IE3YAei"
      },
      "source": [
        "The training files are now ready to be activated. Let's now explore them, starting with `encode.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E78TYMuaYW24"
      },
      "source": [
        "## Step 3: Encoding the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "152DPeHKYYry"
      },
      "source": [
        "The dataset must be encoded before training it.\n",
        "\n",
        "The dataset is loaded, encoded, and saved in out.npz when we run the cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaUzYNI0ZOgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1dcabc2-4314-4326-82e8-24a44199c378"
      },
      "source": [
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "model_name=\"117M\"\n",
        "\n",
        "!python /content/gpt-2/src/encode.py dset.txt out.npz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-20 05:45:18.685383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Reading files\n",
            "100% 1/1 [00:10<00:00, 10.22s/it]\n",
            "Writing out.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsu361O0ZiZE"
      },
      "source": [
        "Our GPT-2 117M model is ready to be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNIk5fi1ZqVh"
      },
      "source": [
        "## Step 4: Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uAosetxZtBj"
      },
      "source": [
        "We will now train the GPT-2 117M model on our dataset. We send the name of our\n",
        "encoded dataset to the program:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M2g07NiaJmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d959940d-bb95-4453-a232-6636a9c2897c"
      },
      "source": [
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "!python train.py --dataset out.npz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-20 05:53:44.657729: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 28, in <module>\n",
            "    import model, sample, encoder\n",
            "  File \"/content/gpt-2/src/model.py\", line 3, in <module>\n",
            "    from tensorflow.contrib.training import HParams\n",
            "ModuleNotFoundError: No module named 'tensorflow.contrib'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C8wDi5sv9vn"
      },
      "source": [
        "When you run the cell, it will train until you stop it. The trained model is saved after 1,000 steps. When the training exceeds 1,000 steps, stop it. The saved model checkpoints are in `/content/gpt-2/src/checkpoint/run1`.\n",
        "\n",
        "You can also stop training the model after 1,000 steps with Ctrl + M.\n",
        "\n",
        "The program manages the optimizer and gradients with the `/content/gpt-2/src/\n",
        "memory_saving_gradients.py` and `/content/gpt-2/src/accumulate.py` programs.\n",
        "\n",
        "`train.py` contains a complete list of parameters that can be tweaked to modify the training process. Run the notebook without changing them first. Then, if you wish, you can experiment with the training parameters and see if you can obtain better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmLCY8Gkw-2h"
      },
      "source": [
        "## Steps 5: Creating a training model directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q700MwulySAO"
      },
      "source": [
        "This section will create a temporary directory for our model, store the information we need, and rename it to replace the directory of the GPT-2 117M model we downloaded.\n",
        "\n",
        "We start by creating a temporary directory named `tgmodel`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzd1XZJwyYVn"
      },
      "source": [
        "run_dir = '/content/gpt-2/models/tgmodel'\n",
        "if not os.path.exists(run_dir):\n",
        "  os.makedirs(run_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpUe40f5ylMg"
      },
      "source": [
        "We then copy the checkpoint files that contain the trained parameters we saved\n",
        "when we trained our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT7tS4r-ylni"
      },
      "source": [
        "!cp /content/gpt-2/src/checkpoint/run1/model-1000.data-00000-of-00001 /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/checkpoint /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-1000.index /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-1000.meta /content/gpt-2/models/tgmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bZ0yzGZywdH"
      },
      "source": [
        "Our `tgmodel` directory now contains the trained parameters of our GPT-2 model.\n",
        "\n",
        "We will now retrieve the hyperparameters and vocabulary files from the GPT-2\n",
        "117M model we downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yRv4DDMzZwM"
      },
      "source": [
        "!cp /content/gpt-2/models/117M/encoder.json /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/models/117M/hparams.json /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/models/117M/vocab.bpe /content/gpt-2/models/tgmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCeWFWVczaXb"
      },
      "source": [
        "Our `tgmodel` directory now contains our complete customized GPT-2 117M model.\n",
        "\n",
        "Our last step is to rename the original GPT-2 model we downloaded and set the\n",
        "name of our model to 117M:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELDMoJaQZjxh"
      },
      "source": [
        "# Renaming the model directories\n",
        "!mv /content/gpt-2/models/117M  /content/gpt-2/models/117M_OpenAI\n",
        "!mv /content/gpt-2/models/tgmodel  /content/gpt-2/models/117M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5ekXFuvZvLB"
      },
      "source": [
        "Our trained model is now the one the cloned OpenAI GPT-2 repository will run.\n",
        "\n",
        "Let's interact with our model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P036P0RNzeTR"
      },
      "source": [
        "## Steps 6: Generating Unconditional Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnkmYUrnzgU1"
      },
      "source": [
        "We will interact with a GPT-2 117M model trained on our dataset. We\n",
        "will first generate an unconditional sample that requires no input on our part. Then we will enter a context paragraph to obtain a conditional text completion response from our trained model.\n",
        "\n",
        "Let's first run an unconditional sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y_5Uektzs_m"
      },
      "source": [
        "os.chdir(\"/content/gpt-2/src\")\n",
        "!python generate_unconditional_samples.py --model_name '117M'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RsuOgC7zzEL"
      },
      "source": [
        "You will not be prompted to enter context sentences since this is an unconditional sample generator.\n",
        "\n",
        ">To stop the cell, double-click on the run button of the cell or type Ctrl + M.\n",
        "\n",
        "The result is random but makes sense from a grammatical perspective. From a\n",
        "semantic point of view, the result is not as interesting because we provided no context. But still, the process is remarkable. It invents posts, writes a title, dates it, invents organizations and addresses, produces a topic, and even imagines web links!\n",
        "\n",
        "The result of an unconditional text generator is interesting but not convincing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdyvOed_1SQM"
      },
      "source": [
        "## Step 7: Interactive Context and Completion Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeRQmHaO1Ukn"
      },
      "source": [
        "We will now run a conditional sample. The context we enter will condition the model to think as we want it to, to complete the text by generating tailor-made paragraphs.\n",
        "\n",
        "If necessary, take a few minutes to go back through the parameters.\n",
        "\n",
        "The program prompts us to enter the context.\n",
        "\n",
        "Let's enter the same paragraph written by Emmanuel Kant.\n",
        "\n",
        "```\n",
        "Human reason, in one sphere of its cognition, is called upon to\n",
        "consider questions, which it cannot decline, as they are presented by\n",
        "its own nature, but which it cannot answer, as they transcend every\n",
        "faculty of the mind.\n",
        "```\n",
        "\n",
        "Run the cell and explore the magic:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osSNLOVg1cLD"
      },
      "source": [
        "os.chdir(\"/content/gpt-2/src\")\n",
        "!python interactive_conditional_samples.py --temperature 0.8 --top_k 40 --model_name '117M'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkNgsReN3yQ1"
      },
      "source": [
        "Wow! I doubt anybody can see the difference between the text completion produced by our trained GPT-2 model and a human. It might also generate different outputs at each run.\n",
        "\n",
        "In fact, I think our model could outperform many humans in this abstract exercise in philosophy, reason, and logic!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fs9EvaUbynJ"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MPhFeMLb0np"
      },
      "source": [
        "We can draw some conclusions from our experiment:\n",
        "\n",
        "- A well-trained transformer model can produce text completion that is\n",
        "human-level.\n",
        "- A GPT-2 model can almost reach human level in text generation on complex\n",
        "and abstract reasoning.\n",
        "- Text context is an efficient way of conditioning a model by demonstrating\n",
        "what is expected.\n",
        "- Text completion is text generation based on text conditioning if context\n",
        "sentences are provided.\n",
        "\n",
        "Bear in mind that our trained GPT-2 model will react like a human. If you enter a short, incomplete, uninteresting, or tricky context, you will obtain puzzled or bad results. GPT-2 expects the best out of us, as in real life!"
      ]
    }
  ]
}