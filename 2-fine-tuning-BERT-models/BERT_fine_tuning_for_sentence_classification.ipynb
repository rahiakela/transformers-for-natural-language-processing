{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-fine-tuning-for-sentence-classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkoc83GcuHDrME+cwHw5yE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/2-fine-tuning-BERT-models/BERT_fine_tuning_for_sentence_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Od8SsghlYp"
      },
      "source": [
        "## BERT Fine-Tuning for Sentence Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTEinuhKh-h6"
      },
      "source": [
        "In this notebook, we will fine-tune a BERT model to predict the downstream task of Acceptability Judgements and measure the predictions with the Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "\n",
        "[Reference Article by Chris McCormick and Nick Ryan](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um7HYzXNiEvW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyVD7l5OiFwD"
      },
      "source": [
        "Pretraining a multi-head attention transformer model requires the parallel\n",
        "processing GPUs can provide.\n",
        "\n",
        "The program first starts by checking if the GPU is activated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPJbHFoKiL4i"
      },
      "source": [
        "%tensorflow_version 2.x     # magic command instructing to use TensorFlow version 2+\n",
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != \"/device:GPU:0\":\n",
        "  raise SystemError(\"GPU device not found\")\n",
        "print(\"Found GPU at: {}\".format(device_name))\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV1yNiCKkWw3"
      },
      "source": [
        "Hugging Face provides modules in TensorFlow and PyTorch. I recommend that a\n",
        "developer feels comfortable with both environments. Excellent AI research teams use either or both environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjPXzXkjjHTb",
        "outputId": "c82ee066-12ac-44bf-b791-873e79928d99"
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0MB 16.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 53.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtPSwZY1kadu"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adDGwVfmkwWz"
      },
      "source": [
        "We will now specify that torch uses the Compute Unified Device Architecture\n",
        "(CUDA) to put the parallel computing power of the NVIDIA card to work for our\n",
        "multi-head attention model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y1TDxAWDkvzt",
        "outputId": "55e0e756-8322-4223-b633-0da53d67e072"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ISlRdkymyUj"
      },
      "source": [
        "%%shell\n",
        "\n",
        "wget https://raw.githubusercontent.com/rahiakela/transformers-for-natural-language-processing/main/2-fine-tuning-BERT-models/in_domain_train.tsv\n",
        "wget https://raw.githubusercontent.com/rahiakela/transformers-for-natural-language-processing/main/2-fine-tuning-BERT-models/out_of_domain_dev.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JGEr_wIlR1P"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yl-lAB1lSmJ"
      },
      "source": [
        "General Language Understanding Evaluation (GLUE) considers Linguistic\n",
        "Acceptability as a top-priority NLP task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wk0BGJVlMSE",
        "outputId": "854b8c1e-3398-401e-dad1-7850067f8f08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# load the datasets\n",
        "df = pd.read_csv(\"in_domain_train.tsv\", delimiter=\"\\t\", header=None, names=[\"sentence_source\", \"label\", \"label_notes\", \"sentence\"])\n",
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8551, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2awaqLAquyJI"
      },
      "source": [
        "A 10-line sample is displayed to visualize the Acceptability Judgment task and see if a sequence makes sense or not:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd8of3fMuvVx",
        "outputId": "86254656-2072-4993-d083-0bf73e204c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4540</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>because john persuaded sally to , he did n't h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7228</th>\n",
              "      <td>sks13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i sent it to you .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6073</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the tuna had been being eaten .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6688</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the cook saved no scraps for the dog .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2315</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the oil separated from the vinegar .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>john hit the stone against the wall .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7529</th>\n",
              "      <td>sks13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>himself likes john .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3568</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>i am anxious for you should study english gram...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7261</th>\n",
              "      <td>sks13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the three sunbathers went swimming .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5641</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the king loved peanut butter cookies .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "4540            ks08  ...  because john persuaded sally to , he did n't h...\n",
              "7228           sks13  ...                                 i sent it to you .\n",
              "6073            c_13  ...                    the tuna had been being eaten .\n",
              "6688            m_02  ...             the cook saved no scraps for the dog .\n",
              "2315            l-93  ...               the oil separated from the vinegar .\n",
              "609             bc01  ...              john hit the stone against the wall .\n",
              "7529           sks13  ...                               himself likes john .\n",
              "3568            ks08  ...  i am anxious for you should study english gram...\n",
              "7261           sks13  ...               the three sunbathers went swimming .\n",
              "5641            c_13  ...             the king loved peanut butter cookies .\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhkOA5YUvOkr"
      },
      "source": [
        "Each sample in the .tsv files contains four tab-separated columns:\n",
        "\n",
        "- Column 1: the source of the sentence (code)\n",
        "- Column 2: the label (0=unacceptable, 1=acceptable)\n",
        "- Column 3: the label annotated by the author\n",
        "- Column 4: the sentence to be classified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o05OaMm8wA3R"
      },
      "source": [
        "## Preparing input for BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UrHeSXawDEN"
      },
      "source": [
        "We will creating sentences, label lists, and adding BERT tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsrQlGCuu6_G",
        "outputId": "d986e926-69bc-4736-f259-df200a5ff319",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Creating sentence, label lists and adding Bert tokens\n",
        "sentences = df.sentence.values\n",
        "\n",
        "# Adding CLS and SEP tokens at the beginning and end of each sentence for BERT\n",
        "sentences = [\"[CLS]\" + sentence + \"[SEP]\" for sentence in sentences]\n",
        "labels = df.label.values\n",
        "\n",
        "sentences[:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"[CLS]our friends wo n't buy this analysis , let alone the next one we propose .[SEP]\",\n",
              " \"[CLS]one more pseudo generalization and i 'm giving up .[SEP]\",\n",
              " \"[CLS]one more pseudo generalization or i 'm giving up .[SEP]\",\n",
              " '[CLS]the more we study verbs , the crazier they get .[SEP]',\n",
              " '[CLS]day by day the facts are getting murkier .[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEga93aGwwEp"
      },
      "source": [
        "## Activating the BERT tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Odzcz8wxhr"
      },
      "source": [
        "We will initialize a pretrained BERT tokenizer. This will save the time\n",
        "it would take to train it from scratch.\n",
        "\n",
        "The program selects an uncased tokenizer, activates it, and displays the first\n",
        "tokenized sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj964v7_wmFm",
        "outputId": "33936b56-a8ae-4b3d-eb9d-303a0487b5ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "print(\"Tokenize the first sentence:\")\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'our', 'friends', 'wo', 'n', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlMkVxpeyUd1"
      },
      "source": [
        "## Processing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKzR-i_MyVRc"
      },
      "source": [
        "We need to determine a fixed maximum length and process the data for the model. The sentences in the datasets are short. But, to make sure of this, the program sets the maximum length of a sequence to 512 and the sequences are padded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZCOiniqxaJv"
      },
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT2pEEMn1dlN"
      },
      "source": [
        "## Creating attention masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiQFaSgH1em6"
      },
      "source": [
        "Now comes a tricky part of the process. We padded the sequences in the previous cell. But we want to prevent the model from performing attention on those padded tokens!\n",
        "\n",
        "The idea is to apply a mask with a value of 1 for each token, which will be followed by 0s for padding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "162P9kvE1c3C"
      },
      "source": [
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i > 0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WSqflqN2HIa"
      },
      "source": [
        "## Splitting data into training and validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLvcn-8j2IEt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPjh91Wm2Ew6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}