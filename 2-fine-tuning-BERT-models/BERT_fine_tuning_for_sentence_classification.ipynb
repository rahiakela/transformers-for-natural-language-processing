{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-fine-tuning-for-sentence-classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOirgP1voGzBcKuq40G6GiX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/2-fine-tuning-BERT-models/BERT_fine_tuning_for_sentence_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Od8SsghlYp"
      },
      "source": [
        "## BERT Fine-Tuning for Sentence Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTEinuhKh-h6"
      },
      "source": [
        "In this notebook, we will fine-tune a BERT model to predict the downstream task of Acceptability Judgements and measure the predictions with the Matthews Correlation Coefficient (MCC).\r\n",
        "\r\n",
        "\r\n",
        "[Reference Article by Chris McCormick and Nick Ryan](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um7HYzXNiEvW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyVD7l5OiFwD"
      },
      "source": [
        "Pretraining a multi-head attention transformer model requires the parallel\r\n",
        "processing GPUs can provide.\r\n",
        "\r\n",
        "The program first starts by checking if the GPU is activated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPJbHFoKiL4i",
        "outputId": "0e5267dd-eb03-44e4-af5a-3f242cd770ed"
      },
      "source": [
        "%tensorflow_version 2.x     # magic command instructing to use TensorFlow version 2+\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != \"/device:GPU:0\":\r\n",
        "  raise SystemError(\"GPU device not found\")\r\n",
        "print(\"Found GPU at: {}\".format(device_name))\r\n",
        "\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x     # magic command instructing to use TensorFlow version 2+`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Found GPU at: /device:GPU:0\n",
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV1yNiCKkWw3"
      },
      "source": [
        "Hugging Face provides modules in TensorFlow and PyTorch. I recommend that a\r\n",
        "developer feels comfortable with both environments. Excellent AI research teams use either or both environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjPXzXkjjHTb",
        "outputId": "8dc618c5-5bdf-4c81-e18e-22da5040445e"
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1MB 8.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 57.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtPSwZY1kadu"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from transformers import BertTokenizer, BertConfig\r\n",
        "from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "from tqdm import tqdm, trange\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import io\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adDGwVfmkwWz"
      },
      "source": [
        "We will now specify that torch uses the Compute Unified Device Architecture\r\n",
        "(CUDA) to put the parallel computing power of the NVIDIA card to work for our\r\n",
        "multi-head attention model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y1TDxAWDkvzt",
        "outputId": "55f99347-99c7-4f25-ac1b-e36016920c0d"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "n_gpu = torch.cuda.device_count()\r\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JGEr_wIlR1P"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yl-lAB1lSmJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wk0BGJVlMSE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}