{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "building-KantaiBERT-from-scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/3-pretraining-RoBERTa-model-from-scratch/building_KantaiBERT_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1oqh0F6W3ad"
      },
      "source": [
        "## Building KantaiBERT from scratch using Transformers and Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y-FvagMdFtL"
      },
      "source": [
        "The Transformer model of this Notebook is a Transformer model named ***KantaiBERT***. ***KantaiBERT*** is trained as a RoBERTa Transformer with DistilBERT architecture. The dataset was compiled with three books by Immanuel Kant downloaded from the [Gutenberg Project](https://www.gutenberg.org/). \n",
        "\n",
        "<center><img src=\"https://eco-ai-horizons.com/data/Kant.jpg\" style=\"margin: auto; display: block; width: 260px;\"></center>\n",
        "\n",
        "![](https://commons.wikimedia.org/wiki/Kant_gemaelde_1.jpg)\n",
        "\n",
        "***KantaiBERT*** was pretrained with a small model of 84 million parameters using the same number of layers and heads as DistilBert, i.e., 6 layers, 768 hidden size,and 12 attention heads. ***KantaiBERT*** is then fine-tuned for a downstream masked Language Modeling task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFILYEDbdcFG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUD724uhdg8_"
      },
      "source": [
        "Notebook edition ([link to original of the reference blogpost](https://huggingface.co/blog/how-to-train)).\n",
        "\n",
        "We will need to install Hugging Face transformers and tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5duRggBRZKvP"
      },
      "source": [
        "# We won't need TensorFlow here\n",
        "!pip uninstall -y tensorflow\n",
        "# Install `transformers` from master\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'\n",
        "# transformers version at notebook update --- 2.9.1\n",
        "# tokenizers version at notebook update --- 0.7.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXYVqKV-FWWU"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "from transformers import RobertaConfig, RobertaTokenizer, RobertaForMaskedLM\n",
        "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments, pipeline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-di6zgTfEmD"
      },
      "source": [
        "## Step 1: Loading the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBYwm-hJf36M"
      },
      "source": [
        "I chose to use the works of Immanuel Kant (1724-1804), the German philosopher, who was the epitome of the Age of Enlightenment. The idea is to introduce human-like logic and pretrained reasoning for downstream reasoning tasks.\n",
        "\n",
        "I compiled the following three books by Immanuel Kant into a text file named `kant.txt`:\n",
        "\n",
        "- The Critique of Pure Reason\n",
        "- The Critique of Practical Reason\n",
        "- Fundamental Principles of the Metaphysic of Morals\n",
        "\n",
        "kant.txt provides a small training dataset to train the transformer model. The result obtained remains experimental. For a real-life project, I would\n",
        "add the complete works of Immanuel Kant, Rene Descartes, Pascal, and Leibnitz, for example.\n",
        "\n",
        "The text file contains the raw text of the books:\n",
        "\n",
        "```\n",
        "…For it is in reality vain to profess _indifference_ in regard to such\n",
        "inquiries, the object of which cannot be indifferent to humanity.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1pUFHnefF1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290ff795-60bd-426b-ce8a-eb27a4b1c066"
      },
      "source": [
        "#1.Load kant.txt using the Colab file manager\n",
        "#2.Downloading the file from GitHub\n",
        "!curl -L https://github.com/rahiakela/transformers-for-natural-language-processing/raw/main/3-pretraining-RoBERTa-model-from-scratch/kant.txt --output \"kant.txt\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   209  100   209    0     0   1088      0 --:--:-- --:--:-- --:--:--  1088\n",
            "100 10.7M  100 10.7M    0     0  10.9M      0 --:--:-- --:--:-- --:--:--  100M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XE-zq89fjIE"
      },
      "source": [
        "## Step 3: Training a Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdAbCELfit3y"
      },
      "source": [
        "Since we does not use a pretrained tokenizer. For example, a pretrained GPT-2 tokenizer could be used. However, the training process includes training a tokenizer from scratch.\n",
        "\n",
        "Hugging Face's `ByteLevelBPETokenizer()` will be trained using `kant.txt`. A bytelevel tokenizer will break a string or word down into a sub-string or sub-word.\n",
        "\n",
        "There are two main advantages among many others:\n",
        "\n",
        "- The tokenizer can break words into minimal components. Then it will merge\n",
        "these small components into statistically interesting ones. For example,\n",
        "\"smaller\" and smallest\" can become \"small,\" \"er,\" and \"est.\" The tokenizer\n",
        "can go further, and we could obtain \"sm\" and \"all,\" for example. In any case,\n",
        "the words are broken down into sub-word tokens and smaller units of subword\n",
        "parts such as \"sm\" and \"all\" instead of simply \"small.\"\n",
        "\n",
        "- The chunks of strings classified as an unknown `unk_token`, using WorkPiece\n",
        "level encoding, will practically disappear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMnymRDLe0hi",
        "outputId": "1d27f920-4aba-48e2-b79d-f10869c3f8a9"
      },
      "source": [
        "%%time \n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.6 s, sys: 396 ms, total: 6 s\n",
            "Wall time: 1.61 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuDM7uInlBMG"
      },
      "source": [
        "The tokenizer will be trained to generate merged sub-string tokens and analyze their frequency.\n",
        "\n",
        "Let's take these two words in the middle of a sentence:\n",
        "\n",
        "```\n",
        "…the tokenizer…\n",
        "```\n",
        "\n",
        "The first step will be to tokenize the string:\n",
        "\n",
        "```\n",
        "'Ġthe', 'Ġtoken', 'izer',\n",
        "```\n",
        "\n",
        "The string is now tokenized into tokens with Ġ (whitespace) information.\n",
        "\n",
        "The next step is to replace them with their indices:\n",
        "\n",
        "| 'Ġthe' | 'Ġtoken' | 'izer' |\n",
        "| ---    | ------   | ----   |\n",
        "| 150    | 5430     | 4712   |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcNN98lil715"
      },
      "source": [
        "## Step 4: Saving the files to disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KAfCLLHnln_"
      },
      "source": [
        "The tokenizer will generate two files when trained:\n",
        "\n",
        "- `merges.txt`, which contains the merged tokenized sub-strings\n",
        "- `vocab.json`, which contains the indices of the tokenized sub-strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqYKX1XYyRI-",
        "outputId": "52065cea-11f5-4b36-a34a-514a088d3b8d"
      },
      "source": [
        "token_dir = '/content/KantaiBERT'\n",
        "if not os.path.exists(token_dir):\n",
        "  os.makedirs(token_dir)\n",
        "tokenizer.save_model('KantaiBERT')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['KantaiBERT/vocab.json', 'KantaiBERT/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI9i_fqtofPM"
      },
      "source": [
        "##  Step 5 Loading the Trained Tokenizer Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7pMWyFtogOt"
      },
      "source": [
        "We could have loaded pretrained tokenizer files. However, we trained our own\n",
        "tokenizer and now are ready to load the files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKVWB8WShT-z"
      },
      "source": [
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./KantaiBERT/vocab.json\",\n",
        "    \"./KantaiBERT/merges.txt\"\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii-sm3BJGhZN"
      },
      "source": [
        "The tokenizer can encode a sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9hQqVS_qZWg",
        "outputId": "48d4974a-e589-455b-ac8a-6cc097a269de"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\").tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'ĠCritique', 'Ġof', 'ĠPure', 'ĠReason', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G43rdmMbGwTc"
      },
      "source": [
        "We can also ask to see the number of tokens in this sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGjAwZVGrfyS",
        "outputId": "a4df4e11-65e1-48b6-ea90-c8d3f4d0f31f"
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2NnA_gOG4G3"
      },
      "source": [
        "The tokenizer now processes the tokens to fit the BERT model variant used. The post processor will add a start and end token, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO5M3vrAhcuj"
      },
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWa8-isaHvwQ"
      },
      "source": [
        "Let's encode a post-processed sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcRkIdnJHwOk",
        "outputId": "c58856ee-4dbe-4733-9045-db63c9b99d7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZCSz_GEH6vC"
      },
      "source": [
        "If we want to see what was added, we can ask the tokenizer to encode the postprocessed sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxokZoF_H8x3",
        "outputId": "e91e74ef-d63d-47cd-c035-c152503bcb5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.encode(\"The Critique of Pure Reason.\").tokens"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'The', 'ĠCritique', 'Ġof', 'ĠPure', 'ĠReason', '.', '</s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Qii9hCIEO9"
      },
      "source": [
        "The output shows that the start and end tokens have been added, which brings the number of tokens to 8 including start and end tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OPFYGG1ILJc"
      },
      "source": [
        "## Step 6: Checking Resource Constraints: GPU and NVIDIA "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0SznLqOIML_"
      },
      "source": [
        "KantaiBERT runs at optimal speed with a Graphics Processing Unit (GPU).\n",
        "\n",
        "We will first run a command to see if an NVIDIA GPU card is present:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD140sFjh0LQ",
        "outputId": "54c31fb1-dedb-46c3-cf98-00307f493425"
      },
      "source": [
        "# Checking Resource Constraints: GPU and NVIDIA \n",
        "!nvidia-smi"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr  1 06:31:35 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nrL_ACjIgU7"
      },
      "source": [
        "We will now check to make sure PyTorch sees CUDA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNZZs-r6iKAV",
        "outputId": "d90b4238-8f3f-4dd4-fc24-8c2359d6bfa8"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ibcXdlYInxX"
      },
      "source": [
        "Compute Unified Device Architecture (CUDA) was developed by NVIDIA to use\n",
        "the parallel computing power of its NVIDIA card."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVOWNZCIIob-"
      },
      "source": [
        "## Step 7: Defining the configuration of the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pYVGTMII0n7"
      },
      "source": [
        "We will be pretraining a RoBERTa-type transformer model using the same number\n",
        "of layers and heads as a DistilBERT transformer. The model will have a vocabulary size set to 52,000, 12 attention heads, and 6 layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTXXutqeDzPi"
      },
      "source": [
        "# Defining the configuration of the Model\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-UsuK9Ps0H7",
        "outputId": "eebfd5e4-5f26-4971-acec-01747bfba98a"
      },
      "source": [
        "print(config)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RobertaConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVyI3gPuI-9X"
      },
      "source": [
        "## Step 8: Re-creating the Tokenizer in Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idze-C0PKMD0"
      },
      "source": [
        "We are now ready to load our trained tokenizer, which is our pretrained tokenizer in `RobertaTokenizer.from_pretained()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4keFBUjQFOD1"
      },
      "source": [
        "# Re-creating the Tokenizer in Transformers\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"./KantaiBERT\", max_length=512)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ildt3MdoKT64"
      },
      "source": [
        "## Step 9: Initializing a Model From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HFU-TA0Khgf"
      },
      "source": [
        "we will initialize a model from scratch and examine the size of the\n",
        "model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzMqR-dzF4Ro",
        "outputId": "4b002b4d-ec27-4856-a3f7-4f53d4b6c87f"
      },
      "source": [
        "# Initializing a Model From Scratch\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "# If we print the model, we can see that it is a BERT model with 6 layers and 12 heads\n",
        "print(model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RobertaForMaskedLM(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): RobertaLMHead(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwtvsuEaLPbJ"
      },
      "source": [
        "The model is small and contains 83,504,416 parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU6JhBSTKiaM",
        "outputId": "61b9cbb2-d3c1-4f7c-d89d-761ff242620e"
      },
      "source": [
        "print(model.num_parameters())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "83504416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkBGRX3NLfWH"
      },
      "source": [
        "**Exploring the Parameters**\n",
        "\n",
        "Let's now look into the parameters. We first store the parameters in LP and calculate the length of the list of parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BXhhe7twTxb"
      },
      "source": [
        "# Exploring the Parameters\n",
        "LP=list(model.parameters())\n",
        "lp=len(LP)\n",
        "print(lp)\n",
        "for p in range(0,lp):\n",
        "  print(LP[p])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjiUxjDoLyMm"
      },
      "source": [
        "The output shows that there are approximately 106 matrices and vectors, which\n",
        "might vary from one transformer model to another.\n",
        "\n",
        "**Counting the parameters**\n",
        "\n",
        "The number of parameters is calculated by taking all parameters in the model and adding them up; for example:\n",
        "\n",
        "- The vocabulary (52,000) x dimensions (768)\n",
        "- The size of many vectors is 1 x 768\n",
        "- The many other dimensions found\n",
        "\n",
        "You will note that $d_{model} = 768$. There are 12 heads in the model. The dimension of $d_k$ for each head will thus be $d_k = \\frac{d_{model}}{12} = 64$. This shows, once again, the optimized\n",
        "Lego concept of the building blocks of a transformer.\n",
        "\n",
        "We will take this further and count the number of parameters of each tensor.\n",
        "\n",
        "First, the program initializes a parameter counter named np (number of parameters) and goes through the lp (108) number of elements in the list of parameters.\n",
        "\n",
        "The parameters are matrices and vectors of different sizes; for example.\n",
        "- 768 x 768\n",
        "- 768 x 1\n",
        "- 768\n",
        "\n",
        "We can see that some parameters are two-dimensional, and some are onedimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej82kG6K3akQ",
        "outputId": "e4d2c6d4-5217-420b-f555-b84b90606a3e"
      },
      "source": [
        "# Counting the parameters\n",
        "np=0\n",
        "for p in range(0, lp):#number of tensors\n",
        "  # An easy way to find out is to try and see if a parameter p in the list LP[p] has two dimensions or not\n",
        "  PL2=True\n",
        "  try:\n",
        "    L2=len(LP[p][0]) #check if 2D\n",
        "  except:\n",
        "    L2=1             #not 2D but 1D\n",
        "    PL2=False\n",
        "  # L1 is the size of the first dimension of the parameter. L3 is the size of the parameters defined by  \n",
        "  L1=len(LP[p])      \n",
        "  L3=L1*L2\n",
        "  # We can now add the parameters up at each step of the loop\n",
        "  np+=L3             # number of parameters per tensor\n",
        "\n",
        "  \"\"\"\n",
        "  We will obtain the sum of the parameters, but we also want to see exactly how the\n",
        "  number of parameters of a transformer model is calculated\n",
        "  \"\"\"\n",
        "  if PL2==True:\n",
        "    print(p,L1,L2,L3)  # displaying the sizes of the parameters\n",
        "  if PL2==False:\n",
        "    print(p,L1,L3)  # displaying the sizes of the parameters\n",
        "\n",
        "print(np)              # total number of parameters"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 52000 768 39936000\n",
            "1 514 768 394752\n",
            "2 1 768 768\n",
            "3 768 768\n",
            "4 768 768\n",
            "5 768 768 589824\n",
            "6 768 768\n",
            "7 768 768 589824\n",
            "8 768 768\n",
            "9 768 768 589824\n",
            "10 768 768\n",
            "11 768 768 589824\n",
            "12 768 768\n",
            "13 768 768\n",
            "14 768 768\n",
            "15 3072 768 2359296\n",
            "16 3072 3072\n",
            "17 768 3072 2359296\n",
            "18 768 768\n",
            "19 768 768\n",
            "20 768 768\n",
            "21 768 768 589824\n",
            "22 768 768\n",
            "23 768 768 589824\n",
            "24 768 768\n",
            "25 768 768 589824\n",
            "26 768 768\n",
            "27 768 768 589824\n",
            "28 768 768\n",
            "29 768 768\n",
            "30 768 768\n",
            "31 3072 768 2359296\n",
            "32 3072 3072\n",
            "33 768 3072 2359296\n",
            "34 768 768\n",
            "35 768 768\n",
            "36 768 768\n",
            "37 768 768 589824\n",
            "38 768 768\n",
            "39 768 768 589824\n",
            "40 768 768\n",
            "41 768 768 589824\n",
            "42 768 768\n",
            "43 768 768 589824\n",
            "44 768 768\n",
            "45 768 768\n",
            "46 768 768\n",
            "47 3072 768 2359296\n",
            "48 3072 3072\n",
            "49 768 3072 2359296\n",
            "50 768 768\n",
            "51 768 768\n",
            "52 768 768\n",
            "53 768 768 589824\n",
            "54 768 768\n",
            "55 768 768 589824\n",
            "56 768 768\n",
            "57 768 768 589824\n",
            "58 768 768\n",
            "59 768 768 589824\n",
            "60 768 768\n",
            "61 768 768\n",
            "62 768 768\n",
            "63 3072 768 2359296\n",
            "64 3072 3072\n",
            "65 768 3072 2359296\n",
            "66 768 768\n",
            "67 768 768\n",
            "68 768 768\n",
            "69 768 768 589824\n",
            "70 768 768\n",
            "71 768 768 589824\n",
            "72 768 768\n",
            "73 768 768 589824\n",
            "74 768 768\n",
            "75 768 768 589824\n",
            "76 768 768\n",
            "77 768 768\n",
            "78 768 768\n",
            "79 3072 768 2359296\n",
            "80 3072 3072\n",
            "81 768 3072 2359296\n",
            "82 768 768\n",
            "83 768 768\n",
            "84 768 768\n",
            "85 768 768 589824\n",
            "86 768 768\n",
            "87 768 768 589824\n",
            "88 768 768\n",
            "89 768 768 589824\n",
            "90 768 768\n",
            "91 768 768 589824\n",
            "92 768 768\n",
            "93 768 768\n",
            "94 768 768\n",
            "95 3072 768 2359296\n",
            "96 3072 3072\n",
            "97 768 3072 2359296\n",
            "98 768 768\n",
            "99 768 768\n",
            "100 768 768\n",
            "101 52000 52000\n",
            "102 768 768 589824\n",
            "103 768 768\n",
            "104 768 768\n",
            "105 768 768\n",
            "83504416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgJZ3d5BRAMX"
      },
      "source": [
        "Note that if a parameter only has one dimension, `PL2=False`, then we only display the first dimension.\n",
        "\n",
        "The total number of parameters of the RoBERTa model is displayed at the end of\n",
        "the list: `83,504,416`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTpHscBRWhw"
      },
      "source": [
        "## Step 10: Building the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS0tFQnFRXh5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlvP_A-THEEl",
        "outputId": "ce117d0d-56d0-473f-eb4e-9efffc7b25dc"
      },
      "source": [
        "# Step 10: Building the Dataset\n",
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./kant.txt\",\n",
        "    block_size=128,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/data/datasets/language_modeling.py:128: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 20.2 s, sys: 661 ms, total: 20.9 s\n",
            "Wall time: 20.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTgWPa9Dipk2"
      },
      "source": [
        "#@title Step 11: Defining a Data Collator\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpvnFFmZJD-N"
      },
      "source": [
        "#@title Step 12: Initializing the Trainer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./KantaiBERT\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "VmaHZXzmkNtJ",
        "outputId": "998acefe-df4b-4d07-8059-d25b323587e1"
      },
      "source": [
        "#@title Step 13: Pre-training the Model\n",
        "%%time\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='2672' max='2672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2672/2672 05:50, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.755200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>4.046900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.770500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.549800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>3.431600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 14s, sys: 1min 37s, total: 5min 51s\n",
            "Wall time: 5min 50s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2672, training_loss=3.8793241306693256, metrics={'train_runtime': 350.6061, 'train_samples_per_second': 7.621, 'total_flos': 1689347110470912, 'epoch': 1.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDNgPls7_l13"
      },
      "source": [
        "#@title Step 14: Saving the Final Model(+tokenizer + config) to disk\n",
        "trainer.save_model(\"./KantaiBERT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltXgXyCbAJLY",
        "outputId": "36ddf0da-9b07-4f97-b8ad-834827e4bc25"
      },
      "source": [
        "#@title Step 15: Language Modeling with the FillMaskPipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./KantaiBERT\",\n",
        "    tokenizer=\"./KantaiBERT\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at ./KantaiBERT and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UIvgZ3S6AO0z",
        "outputId": "19c8ae6c-b5b3-4be9-c84e-772fabc5a5c9"
      },
      "source": [
        "fill_mask(\"Human thinking involves<mask>.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.010303723625838757,\n",
              "  'sequence': '<s>Human thinking involves reason.</s>',\n",
              "  'token': 394,\n",
              "  'token_str': 'Ġreason'},\n",
              " {'score': 0.010289391502737999,\n",
              "  'sequence': '<s>Human thinking involves priori.</s>',\n",
              "  'token': 578,\n",
              "  'token_str': 'Ġpriori'},\n",
              " {'score': 0.009549057111144066,\n",
              "  'sequence': '<s>Human thinking involves conceptions.</s>',\n",
              "  'token': 610,\n",
              "  'token_str': 'Ġconceptions'},\n",
              " {'score': 0.008349979296326637,\n",
              "  'sequence': '<s>Human thinking involves experience.</s>',\n",
              "  'token': 535,\n",
              "  'token_str': 'Ġexperience'},\n",
              " {'score': 0.00743826711550355,\n",
              "  'sequence': '<s>Human thinking involves will.</s>',\n",
              "  'token': 487,\n",
              "  'token_str': 'Ġwill'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    }
  ]
}