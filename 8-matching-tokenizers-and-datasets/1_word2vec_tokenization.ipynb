{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-word2vec-tokenization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPe5bC9alo+0X4TlnKYtTwz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/8-matching-tokenizers-and-datasets/1_word2vec_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKPF6jg1b48O"
      },
      "source": [
        "## Word2Vec tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBULbL1GcC1g"
      },
      "source": [
        "As long as things go well, nobody thinks about pretrained tokenizers. It's like in real life. We can drive a car for years without thinking about the engine. Then, one day our car breaks down, and we try to find the reasons to explain the situation.\n",
        "\n",
        "The same happens with pretrained tokenizers. Sometimes the results are not what\n",
        "we expect. Some word pairs just don't fit together, as we can see.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/tokenizers-miscalculated.png?raw=1' width='800'/>\n",
        "\n",
        "QC refers to Quality Control. In any strategic corporate project, QC is mandatory. The quality of the output will determine the survival of a critical project. If the project is not strategic, errors will sometimes be acceptable. In a strategic project, even a few errors imply a risk management audit's intervention to see if the project should be continued or abandoned.\n",
        "\n",
        "From the perspectives of quality control and risk management, tokenizing datasets that are irrelevant (too many useless words or critical words missing) will confuse the embedding algorithms and produce \"poor results.\" \n",
        "\n",
        "In a strategic AI project, \"poor results\" can be a single error with a dramatic\n",
        "consequence (especially in medical, airplane or rocket assembly, or other critical domains)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTAu4vE0dAmP"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcAtFL6GdBsh"
      },
      "source": [
        "!pip install gensim==3.8.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2fBq01VdGGh"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import gensim \n",
        "from gensim.models import Word2Vec \n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings(action = 'ignore')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hneyMFzZdPnV"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWyHKVVudQFM"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rahiakela/transformers-for-natural-language-processing/main/8-matching-tokenizers-and-datasets/text.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds_ZbUPyewiE"
      },
      "source": [
        "## Train word2vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL2-QzOre1ml"
      },
      "source": [
        "`text.txt`, our dataset, contains the American Declaration of Independence, the Bill of Rights, the Magna Carta, the works of Emmanuel Kant, and other texts.\n",
        "\n",
        "We will now tokenize text.txt and train a word2vec model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBasWNZBejPE"
      },
      "source": [
        "sample = open(\"text.txt\", \"r\")\n",
        "s = sample.read()\n",
        "\n",
        "# processing escape characters\n",
        "f = s.replace(\"\\n\", \" \")\n",
        "\n",
        "data = []\n",
        "# sentence parsing\n",
        "for i in sent_tokenize(f):\n",
        "  temp = []\n",
        "  # tokenize the sentence into words\n",
        "  for j in word_tokenize(i):\n",
        "    temp.append(j.lower())\n",
        "  data.append(temp)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtHp2NAvgDn4"
      },
      "source": [
        "`window=5` is an interesting parameter. It limits the distance between the current word and the predicted word in an input sentence. `sq=1` means a skip-gram training algorithm is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD6zi7uUfkFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b16809-74ec-48a2-a0fd-620afd56caa5"
      },
      "source": [
        "# Creating Skip Gram model\n",
        "model = gensim.models.Word2Vec(data, min_count=1, size=512, window=5, sg=1)\n",
        "print(model)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=11822, size=512, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw6v0ruogTBr"
      },
      "source": [
        "We have a word representation model with embedding and can create a cosine\n",
        "similarity function named similarity(word1,word2). We will send word1 and word2\n",
        "to the function, which will return a cosine similarity value between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQByhpNdf1nF"
      },
      "source": [
        "def similarity(word1, word2):\n",
        "  cosine = False\n",
        "  try:\n",
        "    a = model[word1]\n",
        "    cosine = True\n",
        "  except KeyError:\n",
        "    print(word1, \"word 1:[unk] key not found in dictionary\")\n",
        "\n",
        "  try:\n",
        "    b = model[word2]\n",
        "    cosine = True\n",
        "  except KeyError:\n",
        "    cosine = False   # #both a and b must be true\n",
        "    print(word2, \"word 2:[unk] key not found in dictionary\")\n",
        "\n",
        "  # Cosine similarity will only be calculated if cosine==True, which means that both word1 and word2 are known\n",
        "  if (cosine == True):\n",
        "    b = model[word2]\n",
        "\n",
        "    # compute cosine similarity\n",
        "    dot = np.dot(a, b)\n",
        "    norma = np.linalg.norm(a)\n",
        "    normb = np.linalg.norm(b)\n",
        "    cos = dot / (norma * normb)\n",
        "\n",
        "    aa = a.reshape(1, 512)\n",
        "    ba = b.reshape(1, 512)\n",
        "\n",
        "    cos_lib = cosine_similarity(aa, ba)\n",
        "\n",
        "  if (cosine == False):\n",
        "    cos_lib = 0\n",
        "\n",
        "  return cos_lib"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OgSVehziiBt"
      },
      "source": [
        "## Case 0: Words in the dataset and the dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpPUFkU-iizx"
      },
      "source": [
        "The words \"freedom\" and \"liberty\" are in the dataset and their cosine similarity can be computed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEs4-iwtiSpn",
        "outputId": "6169b680-7eeb-4570-b577-6ef5d0588e54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word1 = \"freedom\"\n",
        "word2 = \"liberty\"\n",
        "\n",
        "print(\"Similarity \", similarity(word1, word2))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity  [[0.38462296]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD_w2_v00aF_"
      },
      "source": [
        "## Case 1: Words not in the dataset or the dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW2oZWus0a-B"
      },
      "source": [
        "Let's now see what happens when a word is missing.\n",
        "\n",
        "A missing word means trouble in many ways. In this case, we send \"corporations\"\n",
        "and \"rights\" to the similarity function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9DD1anT0Eio"
      },
      "source": [
        "word1 = \"corporations\"\n",
        "word2 = \"rights\"\n",
        "\n",
        "print(\"Similarity \", similarity(word1, word2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpklX-uy4NUS"
      },
      "source": [
        "The missing word will provoke a chain of events and problems that will distort the transformer model's output if the word was important. We will refer to the missing word as unk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkW6VXTK5bAr"
      },
      "source": [
        "## Case 2: Noisy relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaVF4Q9k5b-2"
      },
      "source": [
        "In this case, the dataset contained the words \"etext\" and \"declaration\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-55djDVj3nd4",
        "outputId": "b7e656b8-dfad-4074-f5a3-e96d74089ae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word1 = \"etext\"\n",
        "word2 = \"declaration\"\n",
        "\n",
        "print(\"Similarity \", similarity(word1, word2))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity  [[0.55050147]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OzhFT3m6J3e"
      },
      "source": [
        "At a trivial or social media level, everything looks good.\n",
        "\n",
        "However, at a professional level, the result is disastrous!\n",
        "\n",
        "\"declaration\" is a meaningful word related to the actual content of the Declaration of Independence.\n",
        "\n",
        "\"etext\" is part of a preface Project Gutenberg adds to all of its ebooks.\n",
        "\n",
        "This might produce erroneous natural language inferences such as \"etext is a\n",
        "declaration\" when the transformer is asked to generate text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qALeJTBm6aYN"
      },
      "source": [
        "## Case 3: Rare words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtZ-jTIw6bKy"
      },
      "source": [
        "Rare words produce devasting effects of the output of transformers for specific tasks that go beyond trivial applications.\n",
        "\n",
        "Managing rare words extends to many domains of natural language.\n",
        "\n",
        "For example, in this case, we are using the word \"justiciar\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BgUW22w559z",
        "outputId": "02e2f5cc-cb85-4e3b-8164-f7752f5cfc55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word1 = \"justiciar\"\n",
        "word2 = \"judgement\"\n",
        "\n",
        "print(\"Similarity \", similarity(word1, word2))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity  [[0.23178002]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0W4jHrz7RYv"
      },
      "source": [
        "One might think that the word \"justiciar\" is far fetched. The tokenizer extracted it from the Magna Carta, which dates back to the early 13th century.\n",
        "\n",
        "If we implement a transformer model in a law firm to summarize documents or\n",
        "other tasks, we must be careful!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-gi20O07ZuM"
      },
      "source": [
        "## Case 4: Replacing rare words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eZ__Mxk7abw"
      },
      "source": [
        "Replacing rare words represents a project in itself. The work this takes is reserved for specific tasks and projects. If a corporate budget can cover the cost of having a knowledge base in aeronautics, for example, it's worth spending the necessary time querying the tokenized directory to find words it missed.\n",
        "\n",
        "Problems can be grouped by topic, solved, and the knowledge base will be updated\n",
        "regularly.\n",
        "\n",
        "In case 3, we stumbled on the word \"judiciar.\" If we go back to its origin, we can see if it comes from the French Normand language and is the root of the French Latinlike word \"judicaire.\"\n",
        "\n",
        "We could replace the word \"judiciar\" with \"judge,\" which conveys the same metaconcept:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN68Spxi6_i0",
        "outputId": "637bbb90-eda7-4f26-8b3e-e23a6d32483c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word1 = \"judge\"\n",
        "word2 = \"judgement\"\n",
        "\n",
        "print(\"Similarity \", similarity(word1, word2))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity  [[0.19672832]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLok6ikY85gD"
      },
      "source": [
        "We could also keep the work \"justiciar\" but try the modern meaning of the word\n",
        "and compare it to \"judge.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sOh7k9S87yH",
        "outputId": "e7eca196-438c-4586-b586-643be9fa72fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word1 = \"justiciar\"\n",
        "word2 = \"judge\"\n",
        "\n",
        "print(\"Similarity \", similarity(word1, word2))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity  [[0.35883075]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwKOUkdt9HsL"
      },
      "source": [
        "If we are managing a critical legal project,\n",
        "we could have the essential documents that contained rare words of any kind\n",
        "translated into standard English. The transformer's performance with NLP tasks\n",
        "would increase, and the knowledge base of the corporation would progressively\n",
        "increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXgsOKuq9Idr"
      },
      "source": [
        "##Case 5: Entailment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyJbND_p9KvO"
      },
      "source": [
        "In this case, we are interested in words in the dictionary and test them in a fixed order.\n",
        "\n",
        "For example, let's see if \"pay\" + \"debt\" makes sense in our similarity function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqLoam8G89MC",
        "outputId": "b63e1742-210f-4aea-9f07-8dd2d479b2ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word1 = \"pay\"\n",
        "word2 = \"debt\"\n",
        "\n",
        "print(\"Similarity \", similarity(word1, word2))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity  [[0.5159261]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28-aUBwS9jxO"
      },
      "source": [
        "We could check the dataset with several word pairs and check if they mean\n",
        "something.\n",
        "\n",
        "If the cosine similarity is above 0.9, then the email could be stripped\n",
        "of useless information and the content added to the knowledge base dataset of the company."
      ]
    }
  ]
}