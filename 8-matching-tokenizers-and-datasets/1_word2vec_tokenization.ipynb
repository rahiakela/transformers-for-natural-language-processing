{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-word2vec-tokenization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFMm3VLy4OaJfpiU/eqO3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/8-matching-tokenizers-and-datasets/1_word2vec_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKPF6jg1b48O"
      },
      "source": [
        "## Word2Vec tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBULbL1GcC1g"
      },
      "source": [
        "As long as things go well, nobody thinks about pretrained tokenizers. It's like in real life. We can drive a car for years without thinking about the engine. Then, one day our car breaks down, and we try to find the reasons to explain the situation.\n",
        "\n",
        "The same happens with pretrained tokenizers. Sometimes the results are not what\n",
        "we expect. Some word pairs just don't fit together, as we can see.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/tokenizers-miscalculated.png?raw=1' width='800'/>\n",
        "\n",
        "QC refers to Quality Control. In any strategic corporate project, QC is mandatory. The quality of the output will determine the survival of a critical project. If the project is not strategic, errors will sometimes be acceptable. In a strategic project, even a few errors imply a risk management audit's intervention to see if the project should be continued or abandoned.\n",
        "\n",
        "From the perspectives of quality control and risk management, tokenizing datasets that are irrelevant (too many useless words or critical words missing) will confuse the embedding algorithms and produce \"poor results.\" \n",
        "\n",
        "In a strategic AI project, \"poor results\" can be a single error with a dramatic\n",
        "consequence (especially in medical, airplane or rocket assembly, or other critical domains)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTAu4vE0dAmP"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcAtFL6GdBsh"
      },
      "source": [
        "!pip install gensim==3.8.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2fBq01VdGGh"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import gensim \n",
        "from gensim.models import Word2Vec \n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings(action = 'ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hneyMFzZdPnV",
        "outputId": "cbdadbc2-8772-4ba5-8e21-4d0840797a6e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWyHKVVudQFM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}