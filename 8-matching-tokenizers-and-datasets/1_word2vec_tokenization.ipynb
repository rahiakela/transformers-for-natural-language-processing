{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-word2vec-tokenization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNwTiJbWKvO4cbUynveRJjz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/8-matching-tokenizers-and-datasets/1_word2vec_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKPF6jg1b48O"
      },
      "source": [
        "## Word2Vec tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBULbL1GcC1g"
      },
      "source": [
        "As long as things go well, nobody thinks about pretrained tokenizers. It's like in real life. We can drive a car for years without thinking about the engine. Then, one day our car breaks down, and we try to find the reasons to explain the situation.\n",
        "\n",
        "The same happens with pretrained tokenizers. Sometimes the results are not what\n",
        "we expect. Some word pairs just don't fit together, as we can see.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/tokenizers-miscalculated.png?raw=1' width='800'/>\n",
        "\n",
        "QC refers to Quality Control. In any strategic corporate project, QC is mandatory. The quality of the output will determine the survival of a critical project. If the project is not strategic, errors will sometimes be acceptable. In a strategic project, even a few errors imply a risk management audit's intervention to see if the project should be continued or abandoned.\n",
        "\n",
        "From the perspectives of quality control and risk management, tokenizing datasets that are irrelevant (too many useless words or critical words missing) will confuse the embedding algorithms and produce \"poor results.\" \n",
        "\n",
        "In a strategic AI project, \"poor results\" can be a single error with a dramatic\n",
        "consequence (especially in medical, airplane or rocket assembly, or other critical domains)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTAu4vE0dAmP"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcAtFL6GdBsh"
      },
      "source": [
        "!pip install gensim==3.8.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2fBq01VdGGh"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import gensim \n",
        "from gensim.models import Word2Vec \n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings(action = 'ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hneyMFzZdPnV",
        "outputId": "c1815dd0-2e92-49d3-96b3-fb3fe2de703a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWyHKVVudQFM"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rahiakela/transformers-for-natural-language-processing/main/8-matching-tokenizers-and-datasets/text.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds_ZbUPyewiE"
      },
      "source": [
        "## Train word2vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL2-QzOre1ml"
      },
      "source": [
        "`text.txt`, our dataset, contains the American Declaration of Independence, the Bill of Rights, the Magna Carta, the works of Emmanuel Kant, and other texts.\n",
        "\n",
        "We will now tokenize text.txt and train a word2vec model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBasWNZBejPE"
      },
      "source": [
        "sample = open(\"text.txt\", \"r\")\n",
        "s = sample.read()\n",
        "\n",
        "# processing escape characters\n",
        "f = s.replace(\"\\n\", \" \")\n",
        "\n",
        "data = []\n",
        "# sentence parsing\n",
        "for i in sent_tokenize(f):\n",
        "  temp = []\n",
        "  # tokenize the sentence into words\n",
        "  for j in word_tokenize(i):\n",
        "    temp.append(j.lower())\n",
        "  data.append(temp)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtHp2NAvgDn4"
      },
      "source": [
        "`window=5` is an interesting parameter. It limits the distance between the current word and the predicted word in an input sentence. `sq=1` means a skip-gram training algorithm is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD6zi7uUfkFn",
        "outputId": "28b59dba-a56f-4b2f-f01e-97ea2c4f696d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Creating Skip Gram model\n",
        "model = gensim.models.Word2Vec(data, min_count=1, size=512, window=5, sg=1)\n",
        "print(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=11822, size=512, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw6v0ruogTBr"
      },
      "source": [
        "We have a word representation model with embedding and can create a cosine\n",
        "similarity function named similarity(word1,word2). We will send word1 and word2\n",
        "to the function, which will return a cosine similarity value between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQByhpNdf1nF"
      },
      "source": [
        "def similarity(word1, word2):\n",
        "  cosine = False\n",
        "  try:\n",
        "    a = model[word1]\n",
        "    cosine = True\n",
        "  except KeyError:\n",
        "    print(word1, \":[unk] key not found in dictionary\")\n",
        "\n",
        "  try:\n",
        "    b = model[word2]\n",
        "    cosine = True\n",
        "  except KeyError:\n",
        "    cosine = False   # #both a and b must be true\n",
        "    print(word2, \":[unk] key not found in dictionary\")\n",
        "\n",
        "  # Cosine similarity will only be calculated if cosine==True, which means that both word1 and word2 are known\n",
        "  if cosine == True:\n",
        "    b = model[word2]\n",
        "\n",
        "    # compute cosine similarity\n",
        "    dot = np.dot(a, b)\n",
        "    norma = np.linalg.norm(a)\n",
        "    normb = no.linalg.norm(b)\n",
        "    cos = dot / (norma * normb)\n",
        "\n",
        "    aa = a.reshape(1, 512)\n",
        "    ba = b.reshape(1, 512)\n",
        "\n",
        "    cos_lib = cosine_similarity(aa, ba)\n",
        "\n",
        "  if cosine == False:\n",
        "    cos_lib = 0\n",
        "\n",
        "  return cos_lib"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OgSVehziiBt"
      },
      "source": [
        "## Case 0: Words in the dataset and the dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpPUFkU-iizx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEs4-iwtiSpn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}