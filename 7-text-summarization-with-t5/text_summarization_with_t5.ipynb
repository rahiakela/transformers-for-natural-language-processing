{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-summarization-with-t5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOCqWH/oPv/H2owXC9wBO7k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/7-text-summarization-with-t5/text_summarization_with_t5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqE_D_SPgfHZ"
      },
      "source": [
        "## Text summarization with T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDHeIsHmgf3U"
      },
      "source": [
        "NLP summarizing tasks extract succinct parts of a text.We will initialize a T5-large transformer model. Finally, we will see how to use T5 to\n",
        "summarize any type of document, including legal and corporate documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DkTpgTtikri"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH5SBGMpil4v"
      },
      "source": [
        "%%shell\n",
        "\n",
        "pip install transformers\n",
        "pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBPLBtwTimpL"
      },
      "source": [
        "import torch\n",
        "import json \n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "\n",
        "# display the architecture of the model or not when we initialize the model\n",
        "display_architecture=True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lakt0Ht8jql8"
      },
      "source": [
        "If we set display_architecture to True, the structure of the encoder layers, decoder layers, and feedforward sub-layers will be displayed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dGP-kMCj1q6"
      },
      "source": [
        "## Initializing the T5-large transformer model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_4xKSDKj2d8"
      },
      "source": [
        "We will now import the T5-large conditional generation model to generate text and the T5-large tokenizer.\n",
        "\n",
        "Initializing a pretrained tokenizer only takes one line. However, nothing proves that the tokenized dictionary contains all the vocabulary we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBUDCoUYjBNA"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-AzH5tWkOlc"
      },
      "source": [
        "# initializes torch.device with 'cpu', CPU is enough for this notebook\n",
        "device = torch.device('cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJB_y42fkfMO"
      },
      "source": [
        "We are ready to explore the architecture of the T5 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPrgZO6kkhkg"
      },
      "source": [
        "## Exploring the architecture of the T5 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is3Sy2cbkiRq"
      },
      "source": [
        "We will explore the architecture and configuration of a T5-large model.\n",
        "\n",
        "If `display_architecture==true`, we can see the configuration of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVlQ84lnkcx4",
        "outputId": "97152ddd-01a9-439c-d6bc-ac5137aaa2dc"
      },
      "source": [
        "if (display_architecture==True):\n",
        "  print(model.config)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T5Config {\n",
            "  \"_name_or_path\": \"t5-large\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 4096,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 24,\n",
            "  \"num_heads\": 16,\n",
            "  \"num_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L8UK7rlk067"
      },
      "source": [
        "The model is a T5 transformer with 16 heads and 24 layers.\n",
        "\n",
        "We can also see the text-to-text implementation of T5, which adds a prefix to an input sentence to trigger the task to perform. The prefix makes it possible to represent a wide range of tasks in a text-to-text format without modifying the model's parameters. \n",
        "\n",
        "In our case, the prefix is summarization:\n",
        "\n",
        "```json\n",
        "\"task_specific_params\": {\n",
        "    \"summarization\": {\n",
        "      \"early_stopping\": true,\n",
        "      \"length_penalty\": 2.0,\n",
        "      \"max_length\": 200,\n",
        "      \"min_length\": 30,\n",
        "      \"no_repeat_ngram_size\": 3,\n",
        "      \"num_beams\": 4,\n",
        "      \"prefix\": \"summarize: \"\n",
        "    },\n",
        "    ---\n",
        "    ---\n",
        "}\n",
        "```\n",
        "\n",
        "We can see that T5:\n",
        "\n",
        "- Implements beam search, which will expand the four most significant text\n",
        "completion predictions.\n",
        "- Applies early stopping when `num_beam` sentences are completed per batch.\n",
        "- Makes sure not to repeat ngrams equal to `no_repeat_ngram_size`.\n",
        "- Controls the length of the samples with `min_length` and `max_length`.\n",
        "\n",
        "\n",
        "Another interesting parameter is the vocabulary size:\n",
        "\n",
        "```\n",
        "\"vocab_size\": 32128\n",
        "```\n",
        "\n",
        "Vocabulary size is a topic in itself. Too much vocabulary will lead to sparse\n",
        "representations. Too little vocabulary will distort the NLP tasks.\n",
        "\n",
        "We can also see the details of the transformer stacks by simply printing the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww1v10H-kzoW"
      },
      "source": [
        "if (display_architecture==True):\n",
        "  print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNGtdWwKma7a"
      },
      "source": [
        "We can see that the model runs operations on 1,024 features for the attention\n",
        "sub-layers and 4,096 for the inner calculations of the feedforward network sublayer that will produce outputs of 1,024 features. The symmetrical structure of transformers is maintained through all of the layers.\n",
        "\n",
        "You can also choose to select a specific aspect of the model by only running the cells you wish:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvp0yZzznm-_"
      },
      "source": [
        "if (display_architecture==True):\n",
        "  print(model.encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2WYE56fntqa"
      },
      "source": [
        "if (display_architecture==True):\n",
        "  print(model.decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vANpHXHn9un"
      },
      "source": [
        "if display_architecture==True:\n",
        "  print(model.forward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1SOTN43oPiG"
      },
      "source": [
        "We have initialized the T5 transformer. Let's now summarize documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWcoIMXoSGH"
      },
      "source": [
        "## Summarizing documents with T5-large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNR-mStPoSwI"
      },
      "source": [
        "The T5 model has a unified structure, whatever the task is through the prefix + input sequence approach. It may seem simple, but it takes NLP transformer models closer to universal training and zero-shot downstream tasks.\n",
        "\n",
        "We will summarize legal and financial examples. Finally, we will define the limits of the approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw4yShP-oJGx"
      },
      "source": [
        "def summarize(text, ml):\n",
        "  # The context text or ground truth is then stripped of the \\n characters\n",
        "  preprocess_text = text.strip().replace(\"\\n\", \"\")\n",
        "  # We then apply the innovative T5 task prefix \"summarize\" to the input text\n",
        "  t5_prepared_Text = \"summarize: \" + preprocess_text\n",
        "  # We can display the processed (stripped) and prepared text (task prefix)\n",
        "  print(\"Preprocessed and prepared text: \\n\", t5_prepared_Text)\n",
        "\n",
        "  # The text is now encoded to tokens IDs and returns them as torch tensors\n",
        "  tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  # The encoded text is ready to be sent to the model to generate a summary\n",
        "  summary_ids = model.generate(tokenized_text, \n",
        "                               num_beams=4,\n",
        "                               no_repeat_ngram_size=2,\n",
        "                               min_length=30,\n",
        "                               max_length=ml,\n",
        "                               early_stopping=True)\n",
        "  \n",
        "  # The generated output is now decoded with the tokenizer\n",
        "  output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "  return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp_ZDV-NrKyd"
      },
      "source": [
        "Let's now experiment with the T5 model with a general topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-W4mXRnrdov"
      },
      "source": [
        "###A general topic sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkgIko-jrfzs"
      },
      "source": [
        "In this subsection, we will run a text written by Project Gutenberg through the T5 model. We will use the sample to run a test on our summarizing function. You can copy and paste any other text you wish or load a text by adding some code.\n",
        "\n",
        "The goal of the program is to run a few samples to see how T5 works. The input text is the beginning of the Project Gutenberg e-book containing the\n",
        "**Declaration of Independence of the United States of America**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l75lOOjrGx7"
      },
      "source": [
        "text = \"\"\"\n",
        "The United States Declaration of Independence was the first Etext\n",
        "released by Project Gutenberg, early in 1971. The title was stored\n",
        "in an emailed instruction set which required a tape or diskpack be\n",
        "hand mounted for retrieval. The diskpack was the size of a large\n",
        "cake in a cake carrier, cost $1500, and contained 5 megabytes, of\n",
        "which this file took 1-2%. Two tape backups were kept plus one on\n",
        "paper tape. The 10,000 files we hope to have online by the end of\n",
        "2001 should take about 1-2% of a comparably priced drive in 2001.\n",
        "\"\"\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DnhnNwztOEv"
      },
      "source": [
        "We then call our summarize function and send the text we want to summarize and\n",
        "the maximum length of the summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqM7cMJZtIim",
        "outputId": "097ff684-5316-4284-dcab-ed8dc1b8ad06"
      },
      "source": [
        "print(\"Number of characters:\", len(text))\n",
        "summary = summarize(text, 50)\n",
        "print(\"\\n\\nSummarized text: \\n\", summary)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of characters: 530\n",
            "Preprocessed and prepared text: \n",
            " summarize: The United States Declaration of Independence was the first Etextreleased by Project Gutenberg, early in 1971. The title was storedin an emailed instruction set which required a tape or diskpack behand mounted for retrieval. The diskpack was the size of a largecake in a cake carrier, cost $1500, and contained 5 megabytes, ofwhich this file took 1-2%. Two tape backups were kept plus one onpaper tape. The 10,000 files we hope to have online by the end of2001 should take about 1-2% of a comparably priced drive in 2001.\n",
            "\n",
            "\n",
            "Summarized text: \n",
            " the united states declaration of independence was the first etext published by project gutenberg, early in 1971. the 10,000 files we hope to have online by the end of2001 should take about 1-2% of a comparably priced drive in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5Ti-fDUtvmi"
      },
      "source": [
        "The output shows we sent 534 characters, the original text (ground truth) that was preprocessed, and the summary (prediction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJko5rsQtxcD"
      },
      "source": [
        "### The Bill of Rights sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaW_5fCIt1a3"
      },
      "source": [
        "The following sample, taken from the Bill of Rights, is more difficult because it expressed the precise rights of a person."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJcPy_ijtbsD",
        "outputId": "66ec7f47-d2b0-4417-fee0-0b877f6cc47f"
      },
      "source": [
        "text = \"\"\"\n",
        "No person shall be held to answer for a capital, or otherwise infamous\n",
        "crime,\n",
        "unless on a presentment or indictment of a Grand Jury,exceptin cases\n",
        "arising\n",
        "in the land or naval forces, or in the Militia, when in actual service\n",
        "in time of War or public danger; nor shall any person be subject for\n",
        "the same offense to be twice put in jeopardy of life or limb;\n",
        "nor shall be compelled in any criminal case to be a witness against\n",
        "himself,\n",
        "nor be deprived of life, liberty, or property, without due process of\n",
        "law;\n",
        "nor shall private property be taken for public use without just\n",
        "compensation.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Number of characters:\", len(text))\n",
        "summary = summarize(text, 50)\n",
        "print(\"\\n\\nSummarized text: \\n\", summary)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of characters: 588\n",
            "Preprocessed and prepared text: \n",
            " summarize: No person shall be held to answer for a capital, or otherwise infamouscrime,unless on a presentment or indictment of a Grand Jury,exceptin casesarisingin the land or naval forces, or in the Militia, when in actual servicein time of War or public danger; nor shall any person be subject forthe same offense to be twice put in jeopardy of life or limb;nor shall be compelled in any criminal case to be a witness againsthimself,nor be deprived of life, liberty, or property, without due process oflaw;nor shall private property be taken for public use without justcompensation.\n",
            "\n",
            "\n",
            "Summarized text: \n",
            " no person shall be held to answer for a capital, or otherwise infamouscrime unless ona presentment or indictment ofa Grand Jury. nor shall any person be subject for the same offense to be twice put\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f_nr126vJTJ"
      },
      "source": [
        "This sample is significant because it shows the limits that any transformer model or any other NLP model faces when faced with a text such as this one. We cannot just present samples that always work and make a user believe that transformers, no matter how innovative they are, have solved all of the NLP challenges we face.\n",
        "\n",
        "Maybe we should have provided a longer text to summarize, used other parameters,\n",
        "used a larger model, or changed the structure of the T5 model. However, no matter how hard you try to summarize a difficult text with an NLP model, you will always find documents that the model fails to summarize.\n",
        "\n",
        "When a model fails on a task, we need to be humble and admit it. The SuperGLUE\n",
        "human baseline is a difficult one to beat. We need to be patient, work harder, and improve transformer models until they can perform better than they do today. There is still room for a lot of progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4j2ExMavi_Z"
      },
      "source": [
        "###A corporate law sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogjaWXXDvj4-"
      },
      "source": [
        "Corporate law contains many legal subtleties, which makes a summarizing task\n",
        "quite tricky.\n",
        "\n",
        "The input of this sample is an excerpt of the corporate law in the state of Montana, USA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qZIyMkfvpcY",
        "outputId": "98c62870-b666-4f1b-eae0-6f62914c4c07"
      },
      "source": [
        "#Montana Corporate Law\n",
        "#https://corporations.uslegal.com/state-corporation-law/montanacorporation-law/#:~:text=Montana%20Corporation%20Law,carrying%20out%20its%20business%20activities.\n",
        "text = \"\"\"\n",
        "The law regarding corporations prescribes that a corporation\n",
        "can be incorporated in the state of Montana to serve any lawful\n",
        "purpose. In the state of Montana, a corporation has all the powers\n",
        "of a natural person for carrying out its business activities. The\n",
        "corporation can sue and be sued in its corporate name. It has\n",
        "perpetual succession. The corporation can buy, sell or otherwise\n",
        "acquire an interest in a real or personal property. It can conduct\n",
        "business, carry on operations, and have offices and exercise the powers\n",
        "in a state, territory or district in possession of the U.S., or in a\n",
        "foreign country. It can appoint officers and agents of the corporation\n",
        "for various duties and fix their compensation.\n",
        "The name of a corporation must contain the word \"corporation\" or\n",
        "its abbreviation \"corp.\" The name of a corporation should not be\n",
        "deceptively similar to the name of another corporation incorporated\n",
        "in the same state. It should not be deceptively identical to the\n",
        "fictitious name adopted by a foreign corporation having business\n",
        "transactions in the state.\n",
        "The corporation is formed by one or more natural persons by executing\n",
        "and filing articles of incorporation to the secretary of state of\n",
        "filing. The qualifications for directors are fixed either by articles\n",
        "of incorporation or bylaws. The names and addresses of the initial\n",
        "directors and purpose of incorporation should be set forth in the\n",
        "articles of incorporation. The articles of incorporation should\n",
        "contain the corporate name, the number of shares authorized to issue,\n",
        "a brief statement of the character of business carried out by the\n",
        "corporation, the names and addresses of the directors until successors\n",
        "are elected, and name and addresses of incorporators. The shareholders\n",
        "have the power to change the size of board of directors.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Number of characters:\", len(text))\n",
        "summary = summarize(text, 50)\n",
        "print(\"\\n\\nSummarized text: \\n\", summary)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of characters: 1805\n",
            "Preprocessed and prepared text: \n",
            " summarize: The law regarding corporations prescribes that a corporationcan be incorporated in the state of Montana to serve any lawfulpurpose. In the state of Montana, a corporation has all the powersof a natural person for carrying out its business activities. Thecorporation can sue and be sued in its corporate name. It hasperpetual succession. The corporation can buy, sell or otherwiseacquire an interest in a real or personal property. It can conductbusiness, carry on operations, and have offices and exercise the powersin a state, territory or district in possession of the U.S., or in aforeign country. It can appoint officers and agents of the corporationfor various duties and fix their compensation.The name of a corporation must contain the word \"corporation\" orits abbreviation \"corp.\" The name of a corporation should not bedeceptively similar to the name of another corporation incorporatedin the same state. It should not be deceptively identical to thefictitious name adopted by a foreign corporation having businesstransactions in the state.The corporation is formed by one or more natural persons by executingand filing articles of incorporation to the secretary of state offiling. The qualifications for directors are fixed either by articlesof incorporation or bylaws. The names and addresses of the initialdirectors and purpose of incorporation should be set forth in thearticles of incorporation. The articles of incorporation shouldcontain the corporate name, the number of shares authorized to issue,a brief statement of the character of business carried out by thecorporation, the names and addresses of the directors until successorsare elected, and name and addresses of incorporators. The shareholdershave the power to change the size of board of directors.\n",
            "\n",
            "\n",
            "Summarized text: \n",
            " in the state of Montana, a corporation can sue and be sued in its corporate name. it can buy, sell or otherwiseacquire an interest in real or personalproperty. the name of the corporation must\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtddx-d_v-3r"
      },
      "source": [
        "This time, T5 found some of the essential aspects of the text to summarize. Take\n",
        "some time to try to incorporate samples of your own to see what happens. Play\n",
        "with the parameters to see if it affects the outcome."
      ]
    }
  ]
}