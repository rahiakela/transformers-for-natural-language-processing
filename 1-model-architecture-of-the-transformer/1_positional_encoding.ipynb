{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-positional-encoding.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLicwGhXQbF0SFPNBkQp1e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/1-model-architecture-of-the-transformer/1_positional_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV-aGZKVa1bx"
      },
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKUJ8dyza2YR"
      },
      "source": [
        "The Transformer's subsequent layers do not start empty-handed. They have learned word embeddings that already provide information on how the words can be associated.\r\n",
        "\r\n",
        "However, a big chunk of information is missing because no additional vector or information indicates a word's position in a sequence.\r\n",
        "\r\n",
        "The designers of the Transformer came up with yet another innovative feature: positional encoding.\r\n",
        "\r\n",
        "Let's see how positional encoding works.\r\n",
        "\r\n",
        "We enter this positional encoding function of the Transformer with no idea of the position of a word in a sequence:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/position-encoding.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "We cannot create independent positional vectors that would have a high cost on the training speed of the Transformer and make attention sub-layers very complex to work with. \r\n",
        "\r\n",
        "**The idea is to add a positional encoding value to the input embedding instead of having additional vectors to describe the position of a token in a sequence.**\r\n",
        "\r\n",
        "We also know that the Transformer expects a fixed size $d_{model} = 512$ (or other constant value for the model) for each vector of the output of the positional encoding function.\r\n",
        "\r\n",
        "If we go back to the sentence we used in the word embedding sub-layer, we can see that **black** and **brown** may be similar, but they are far apart:\r\n",
        "\r\n",
        "```\r\n",
        "The black cat sat on the couch and the brown dog slept on the rug.\r\n",
        "```\r\n",
        "\r\n",
        "The word **black** is in position 2, pos=2, and the word **brown** is in position 10, pos=10.\r\n",
        "\r\n",
        "Our problem is to find a way to add a value to the word embedding of each word so that it has that information. However, we need to add a value to the $d_{model} = 512$ dimensions! For each word embedding vector, we need to find a way to provide information to $i$ in the $range(0,512)$ dimensions of the word embedding vector of **black** and **brown**.\r\n",
        "\r\n",
        "There are many ways to achieve this goal. The designers found a clever way to use a unit sphere to represent positional encoding with sine and cosine values that will thus remain small but very useful.\r\n",
        "\r\n",
        "Vaswani et al. (2017) provide sine and cosine functions so that we can generate different frequencies for the positional encoding (PE) for each position and each dimension i of the dmodel = 512 of the word embedding vector:\r\n",
        "\r\n",
        "$$ PE_{(pos,2i)}=sin(\\frac{pos}{10000\\frac{2i}{d_{model}}}) $$"
      ]
    }
  ]
}