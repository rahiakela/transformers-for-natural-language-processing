{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-positional-encoding.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMUx6Ws+6SyJg6B2PEm8Cev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/1-model-architecture-of-the-transformer/1_positional_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV-aGZKVa1bx"
      },
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKUJ8dyza2YR"
      },
      "source": [
        "The Transformer's subsequent layers do not start empty-handed. They have learned word embeddings that already provide information on how the words can be associated.\r\n",
        "\r\n",
        "However, a big chunk of information is missing because no additional vector or information indicates a word's position in a sequence.\r\n",
        "\r\n",
        "The designers of the Transformer came up with yet another innovative feature: positional encoding.\r\n",
        "\r\n",
        "Let's see how positional encoding works.\r\n",
        "\r\n",
        "We enter this positional encoding function of the Transformer with no idea of the position of a word in a sequence:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/position-encoding.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "We cannot create independent positional vectors that would have a high cost on the training speed of the Transformer and make attention sub-layers very complex to work with. \r\n",
        "\r\n",
        "**The idea is to add a positional encoding value to the input embedding instead of having additional vectors to describe the position of a token in a sequence.**\r\n",
        "\r\n",
        "We also know that the Transformer expects a fixed size $d_{model} = 512$ (or other constant value for the model) for each vector of the output of the positional encoding function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWx2nmuEquOE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w60vlkZqv_X"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrrUFXTqyAZB"
      },
      "source": [
        "import torch\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaejOHauyC9S"
      },
      "source": [
        "import math\r\n",
        "import numpy as np\r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \r\n",
        "import gensim \r\n",
        "from gensim.models import Word2Vec \r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings \r\n",
        "warnings.filterwarnings(action = 'ignore') "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MRDaB3YyIbB"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rahiakela/transformers-for-natural-language-processing/main/1-model-architecture-of-the-transformer/text.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhtUSH4YDjN1"
      },
      "source": [
        "dprint=0 # prints outputs if set to 1, default=0\r\n",
        "\r\n",
        "# loading txt file\r\n",
        "sample = open(\"text.txt\", \"r\")\r\n",
        "s = sample.read()\r\n",
        "\r\n",
        "# processing escape characters\r\n",
        "f = s.replace(\"\\n\", \" \")\r\n",
        "\r\n",
        "data = []\r\n",
        "# sentence parsing\r\n",
        "for i in sent_tokenize(f):\r\n",
        "  temp = []\r\n",
        "  # tokenize the sentence into words\r\n",
        "  for j in word_tokenize(i):\r\n",
        "    temp.append(j.lower())\r\n",
        "  data.append(temp)\r\n",
        "\r\n",
        "# Creating Skip Gram model \r\n",
        "model = gensim.models.Word2Vec(data, min_count=1, size=512, window=5, sg=1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOQ38LVVF3U8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a8daf1-1266-4e64-8263-83b9dca27cdc"
      },
      "source": [
        "# 1-The 2-black 3-cat 4-sat 5-on 6-the 7-couch 8-and 9-the 10-brown 11-dog 12-slept 13-on 14-the 15-rug.\r\n",
        "word1 = \"black\"\r\n",
        "word2 = \"brown\"\r\n",
        "\r\n",
        "pos1 = 2\r\n",
        "pos2 = 10\r\n",
        "\r\n",
        "a = model[word1]\r\n",
        "b = model[word2]\r\n",
        "\r\n",
        "if dprint == 1:\r\n",
        "  print(a)\r\n",
        "\r\n",
        "# compute cosine similarity\r\n",
        "dot = np.dot(a, b)\r\n",
        "norm_a = np.linalg.norm(a)\r\n",
        "norm_b = np.linalg.norm(b)\r\n",
        "\r\n",
        "cos = dot / (norm_a * norm_b)\r\n",
        "\r\n",
        "aa = a.reshape(1, 512)\r\n",
        "bb = b.reshape(1, 512)\r\n",
        "cos_lib = cosine_similarity(aa, bb)\r\n",
        "cos_lib"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99987805]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3zy-yLXqwTo"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gj2Jg0WqyqL"
      },
      "source": [
        "If we go back to the sentence we used in the word embedding sub-layer, we can see that **black** and **brown** may be similar, but they are far apart:\r\n",
        "\r\n",
        "```\r\n",
        "The black cat sat on the couch and the brown dog slept on the rug.\r\n",
        "```\r\n",
        "\r\n",
        "The word **black** is in position 2, pos=2, and the word **brown** is in position 10, pos=10.\r\n",
        "\r\n",
        "Our problem is to find a way to add a value to the word embedding of each word so that it has that information. However, we need to add a value to the $d_{model} = 512$ dimensions! For each word embedding vector, we need to find a way to provide information to $i$ in the $range(0,512)$ dimensions of the word embedding vector of **black** and **brown**.\r\n",
        "\r\n",
        "There are many ways to achieve this goal. The designers found a clever way to use a unit sphere to represent positional encoding with sine and cosine values that will thus remain small but very useful.\r\n",
        "\r\n",
        "Vaswani et al. (2017) provide sine and cosine functions so that we can generate different frequencies for the positional encoding (PE) for each position and each dimension i of the dmodel = 512 of the word embedding vector:\r\n",
        "\r\n",
        "$$ PE_{(pos,2i)}=sin\\begin{pmatrix} \\frac{pos}{10000\\frac{2i}{d_{model}}} \\end{pmatrix}$$\r\n",
        "\r\n",
        "$$ PE_{(pos,2i+1)}=cos\\begin{pmatrix} \\frac{pos}{10000\\frac{2i}{d_{model}}} \\end{pmatrix}$$\r\n",
        "\r\n",
        "If we start at the beginning of the word embedding vector, we will begin with a constant (512), $i=0$, and end with $i=511$. **This means that the sine function will be applied to the even numbers and the cosine function to the odd numbers**. Some implementations do it differently. In that case, the domain of the sine function can be $i \\in [0, 255]$  and the domain of the cosine function can be $i \\in [256, 512]$. This will produce similar results.\r\n",
        "\r\n",
        "A literal translation into Python produces the following code for a positional vector $pe[0][i]$ for a position `pos`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWbYK39rg2y"
      },
      "source": [
        "def positional_encoding(pos, pe):\r\n",
        "  for i in range(0, 512, 2):\r\n",
        "    pe[0][i] = math.sin(pos / (10000 ** ((2 * i) / 512)))\r\n",
        "    pe[0][i + 1] = math.cos(pos / (10000 ** ((2 * i) / 512)))\r\n",
        "  return pe"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPhTFVxN91T-"
      },
      "source": [
        "Before going further, you might want to see the plot of the sine function, for example, for pos=2.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/pos-2-plot.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "Before going further, you might want to see the plot of the sine function, for example, for pos=10.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/pos-10-plot.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "If we go back to the sentence we are parsing in this section, we can see that black is in position pos=2 and brown is in position pos=10:\r\n",
        "\r\n",
        "```\r\n",
        "The black cat sat on the couch and the brown dog slept on the rug.\r\n",
        "```\r\n",
        "\r\n",
        "If we apply the sine and cosine functions literally for pos=2, we obtain a size=512 positional encoding vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCoRd1QN9jKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14811b27-36fd-4267-c052-7f64903a39a8"
      },
      "source": [
        "d_model=512\r\n",
        "max_length=20\r\n",
        "max_len=max_length\r\n",
        "\r\n",
        "pe = torch.zeros(max_len, d_model)\r\n",
        "positional_encoding(2, pe)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9.0930e-01, -4.1615e-01,  9.5814e-01,  ...,  1.0000e+00,\n",
              "          2.1492e-08,  1.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        ...,\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O58-bxOYBXES"
      },
      "source": [
        "We also obtain a size=512, positional encoding vector for position 10, pos=10:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg2OE-yw9wN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35237387-154d-4d80-9f00-43a08c437486"
      },
      "source": [
        "positional_encoding(10, pe)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.4402e-01, -8.3907e-01,  1.1878e-01,  ...,  1.0000e+00,\n",
              "          1.0746e-07,  1.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        ...,\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTtqNgDvCbkM"
      },
      "source": [
        "When we look at the results we obtained with an intuitive literal translation, we would now like to check whether the results are meaningful.\r\n",
        "\r\n",
        "The cosine similarity function used for word embedding comes in handy for having\r\n",
        "a better visualization of the proximity of the positions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a9W-itaBKCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f173afdb-e79c-4347-9d57-c6743f92635c"
      },
      "source": [
        "cosine_similarity(positional_encoding(2, pe), positional_encoding(10, pe))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmUBhy57HhKU"
      },
      "source": [
        "The similarity between the position of the words black and brown and the lexical\r\n",
        "field (groups of words that go together) similarity is different:\r\n",
        "\r\n",
        "```python\r\n",
        "cosine_similarity(black, brown)= [[0.9998901]]\r\n",
        "```\r\n",
        "\r\n",
        "The encoding of the position shows a lower similarity value than the word\r\n",
        "embedding similarity.\r\n",
        "\r\n",
        "The positional encoding has taken these words apart. Bear in mind that word\r\n",
        "embeddings will vary with the corpus used to train them.\r\n",
        "\r\n",
        "**The problem is now how to add the positional encoding to the word embedding\r\n",
        "vectors.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sXkbymkH2dw"
      },
      "source": [
        "## Adding positional encoding to the embedding vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOw1BUCrH3YO"
      },
      "source": [
        "The authors of the Transformer found a simple way by merely adding the positional encoding vector to the word embedding vector:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/position-encoding2.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "If we go back and take the word embedding of black, for example, and name it\r\n",
        "$y_1=black$, we are ready to add it to the positional vector $pe(2)$ we obtained with positional encoding functions. We will obtain the positional encoding $pc(black)$ of the input word black:\r\n",
        "\r\n",
        "$$ pc(black)=y_1 + pe(2)$$\r\n",
        "\r\n",
        "The solution is straightforward. **However, if we apply it as shown, we might lose the information of the word embedding, which will be minimized by the positional encoding vector.**\r\n",
        "\r\n",
        "There are many possibilities to increase the value of $y_1$ to make sure that the information of the word embedding layer can be used efficiently in the subsequent layers.\r\n",
        "\r\n",
        "One of the many possibilities is to add an arbitrary value to $y_1$, the word embedding of black:\r\n",
        "\r\n",
        "$$y_1 * math.sqrt(d_{model})$$\r\n",
        "\r\n",
        "We can now add the positional vector to the embedding vector of the word black,\r\n",
        "both of which are the same size 512.\r\n",
        "\r\n",
        "```python\r\n",
        "for i in range(0, 512,2):\r\n",
        "  pe[0][i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\r\n",
        "  pc[0][i] = (y[0][i]*math.sqrt(d_model))+ pe[0][i]\r\n",
        "  pe[0][i+1] = math.cos(pos / (10000 ** ((2 * i)/d_model)))\r\n",
        "  pc[0][i+1] = (y[0][i+1]*math.sqrt(d_model))+ pe[0][i+1]\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVY1w0lgQUun"
      },
      "source": [
        "Let's define some parameters and variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6GLLWG9QbBs"
      },
      "source": [
        "pe1=aa.copy()  # pe for 1\r\n",
        "pe2=aa.copy()  # pe for 2\r\n",
        "pe3=aa.copy()  # pe for 3\r\n",
        "paa=aa.copy()  # y for 1\r\n",
        "pba=bb.copy()  # y for 2\r\n",
        "\r\n",
        "d_model=512\r\n",
        "max_print=d_model\r\n",
        "max_length=20"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY9SZC-dHa7V",
        "outputId": "8cf03e96-bcc9-4b86-e2f3-7494e57be4da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(0, max_print, 2):\r\n",
        "  pe1[0][i] = math.sin(pos1 / (10000 ** ((2 * i) / d_model)))\r\n",
        "  paa[0][i] = (paa[0][i] * math.sqrt(d_model)) + pe1[0][i]\r\n",
        "\r\n",
        "  pe1[0][i + 1] = math.cos(pos1 / (10000 ** ((2 * i) / d_model)))\r\n",
        "  paa[0][i + 1] = (paa[0][i + 1] * math.sqrt(d_model)) + pe1[0][i + 1]\r\n",
        "\r\n",
        "  if dprint == 1:\r\n",
        "    print(i, pe1[0][i], i + 1, pe1[0][i + 1])\r\n",
        "    print(i, paa[0][i], i + 1, paa[0][i + 1])\r\n",
        "    print(\"\\n\")\r\n",
        "\r\n",
        "max_len = max_length\r\n",
        "pe = torch.zeros(max_len, d_model)\r\n",
        "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\r\n",
        "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\r\n",
        "pe[:, 0::2] = torch.sin(position * div_term)\r\n",
        "pe[:, 1::2] = torch.cos(position * div_term)\r\n",
        "\r\n",
        "print(pe[:, 0::2])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 8.4147e-01,  8.2186e-01,  8.0196e-01,  ...,  1.1140e-04,\n",
            "          1.0746e-04,  1.0366e-04],\n",
            "        [ 9.0930e-01,  9.3641e-01,  9.5814e-01,  ...,  2.2279e-04,\n",
            "          2.1492e-04,  2.0733e-04],\n",
            "        ...,\n",
            "        [-9.6140e-01, -6.3753e-01, -1.1153e-01,  ...,  1.8938e-03,\n",
            "          1.8268e-03,  1.7623e-03],\n",
            "        [-7.5099e-01, -9.9638e-01, -8.6358e-01,  ...,  2.0052e-03,\n",
            "          1.9343e-03,  1.8659e-03],\n",
            "        [ 1.4988e-01, -4.9773e-01, -9.2024e-01,  ...,  2.1165e-03,\n",
            "          2.0418e-03,  1.9696e-03]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t30OyJIJP8-I",
        "outputId": "3b4edc67-4e5e-452a-c6b4-aaf848050e8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(pe[:, 1::2])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "        [ 0.5403,  0.5697,  0.5974,  ...,  1.0000,  1.0000,  1.0000],\n",
            "        [-0.4161, -0.3509, -0.2863,  ...,  1.0000,  1.0000,  1.0000],\n",
            "        ...,\n",
            "        [-0.2752, -0.7704, -0.9938,  ...,  1.0000,  1.0000,  1.0000],\n",
            "        [ 0.6603,  0.0850, -0.5042,  ...,  1.0000,  1.0000,  1.0000],\n",
            "        [ 0.9887,  0.8673,  0.3914,  ...,  1.0000,  1.0000,  1.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re-DEs2VRjcA"
      },
      "source": [
        "The same operation is applied to the word brown and all of the other words in a\r\n",
        "sequence. The output of this algorithm, which is not rule-based, might slightly vary during each run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsCkunRPRAkZ"
      },
      "source": [
        "for i in range(0, max_print, 2):\r\n",
        "  pe2[0][i] = math.sin(pos2 / (10000 ** ((2 * i) / d_model)))\r\n",
        "  pba[0][i] = (pba[0][i] * math.sqrt(d_model)) + pe2[0][i]\r\n",
        "\r\n",
        "  pe2[0][i + 1] = math.cos(pos2 / (10000 ** ((2 * i) / d_model)))\r\n",
        "  pba[0][i + 1] = (pba[0][i + 1] * math.sqrt(d_model)) + pe2[0][i + 1]\r\n",
        "\r\n",
        "  if dprint == 1:\r\n",
        "    print(i, pe2[0][i], i + 1, pe2[0][i + 1])\r\n",
        "    print(i, pba[0][i], i + 1, pba[0][i + 1])\r\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmHCTvgmTKse"
      },
      "source": [
        "We can apply the cosine similarity function to the positional encoding vectors of black and brown:\r\n",
        "\r\n",
        "```python\r\n",
        "cosine_similarity(pc(black), pc(brown)= [[0.9627094]]\r\n",
        "```\r\n",
        "\r\n",
        "We now have a clear view of the positional encoding process through the three\r\n",
        "cosine similarity functions we applied to the three states representing the words black and brown:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9XsBCzZTB-w",
        "outputId": "e201ee04-d184-4bd1-adba-9c8e2cbe2577",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(word1, word2)\r\n",
        "\r\n",
        "cos_lib = cosine_similarity(aa, bb)\r\n",
        "print(cos_lib, \" >> word similarity\")\r\n",
        "\r\n",
        "cos_lib = cosine_similarity(pe1, pe2)\r\n",
        "print(cos_lib, \"  >> positional similarity\")\r\n",
        "\r\n",
        "cos_lib = cosine_similarity(paa, pba)\r\n",
        "print(cos_lib, \"  >> positional encoding similarity\")\r\n",
        "\r\n",
        "if dprint == 1:\r\n",
        "  print(word1)\r\n",
        "  print(\"embedding\")\r\n",
        "  print(aa)\r\n",
        "  print(\"positional encoding\")\r\n",
        "  print(pe1)\r\n",
        "  print(\"encoded embedding\")\r\n",
        "  print(paa)\r\n",
        "\r\n",
        "  print(word2)\r\n",
        "  print(\"embedding\")\r\n",
        "  print(ba)\r\n",
        "  print(\"positional encoding\")\r\n",
        "  print(pe2)\r\n",
        "  print(\"encoded embedding\")\r\n",
        "  print(pba)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "black brown\n",
            "[[0.99987805]]  >> word similarity\n",
            "[[0.8600013]]   >> positional similarity\n",
            "[[0.9608184]]   >> positional encoding similarity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqo9VWVEUl3A"
      },
      "source": [
        "We saw that the initial word similarity of their embeddings was very high, with a value of 0.99. Then we saw the positional encoding vector of positions 2 and\r\n",
        "10 drew these two words apart with a lower similarity value of 0.86.\r\n",
        "\r\n",
        "Finally, we added the word embedding vector of each word to its respective\r\n",
        "positional encoding vector. We saw that this brought the cosine similarity of the two words to 0.96.\r\n",
        "\r\n",
        "**The positional encoding of each word now contains the initial word embedding\r\n",
        "information and the positional encoding values.**\r\n",
        "\r\n",
        "**The output of positional encoding is the multi-head attention sub-layer.**"
      ]
    }
  ]
}