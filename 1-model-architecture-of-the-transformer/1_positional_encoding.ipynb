{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-positional-encoding.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOqpZ8RZ3M8uzqdLhJ0793C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/1-model-architecture-of-the-transformer/1_positional_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV-aGZKVa1bx"
      },
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKUJ8dyza2YR"
      },
      "source": [
        "The Transformer's subsequent layers do not start empty-handed. They have learned word embeddings that already provide information on how the words can be associated.\r\n",
        "\r\n",
        "However, a big chunk of information is missing because no additional vector or information indicates a word's position in a sequence.\r\n",
        "\r\n",
        "The designers of the Transformer came up with yet another innovative feature: positional encoding.\r\n",
        "\r\n",
        "Let's see how positional encoding works.\r\n",
        "\r\n",
        "We enter this positional encoding function of the Transformer with no idea of the position of a word in a sequence:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/position-encoding.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "We cannot create independent positional vectors that would have a high cost on the training speed of the Transformer and make attention sub-layers very complex to work with. \r\n",
        "\r\n",
        "**The idea is to add a positional encoding value to the input embedding instead of having additional vectors to describe the position of a token in a sequence.**\r\n",
        "\r\n",
        "We also know that the Transformer expects a fixed size $d_{model} = 512$ (or other constant value for the model) for each vector of the output of the positional encoding function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWx2nmuEquOE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w60vlkZqv_X"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrrUFXTqyAZB",
        "outputId": "3d1ba78a-8310-4b79-92d2-9b55fa02db87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaejOHauyC9S"
      },
      "source": [
        "import math\r\n",
        "import numpy as np\r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \r\n",
        "import gensim \r\n",
        "from gensim.models import Word2Vec \r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings \r\n",
        "warnings.filterwarnings(action = 'ignore') "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MRDaB3YyIbB"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rahiakela/transformers-for-natural-language-processing/main/1-model-architecture-of-the-transformer/text.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhtUSH4YDjN1"
      },
      "source": [
        "dprint=0 # prints outputs if set to 1, default=0\r\n",
        "\r\n",
        "# loading txt file\r\n",
        "sample = open(\"text.txt\", \"r\")\r\n",
        "s = sample.read()\r\n",
        "\r\n",
        "# processing escape characters\r\n",
        "f = s.replace(\"\\n\", \" \")\r\n",
        "\r\n",
        "data = []\r\n",
        "# sentence parsing\r\n",
        "for i in sent_tokenize(f):\r\n",
        "  temp = []\r\n",
        "  # tokenize the sentence into words\r\n",
        "  for j in word_tokenize(i):\r\n",
        "    temp.append(j.lower())\r\n",
        "  data.append(temp)\r\n",
        "\r\n",
        "# Creating Skip Gram model \r\n",
        "model = gensim.models.Word2Vec(data, min_count=1, size=512, window=5, sg=1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOQ38LVVF3U8",
        "outputId": "ff59d151-40e6-4fb0-9841-4636f228806e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 1-The 2-black 3-cat 4-sat 5-on 6-the 7-couch 8-and 9-the 10-brown 11-dog 12-slept 13-on 14-the 15-rug.\r\n",
        "word1 = \"black\"\r\n",
        "word2 = \"brown\"\r\n",
        "\r\n",
        "pos1 = 2\r\n",
        "pos2 = 10\r\n",
        "\r\n",
        "a = model[word1]\r\n",
        "b = model[word2]\r\n",
        "\r\n",
        "if dprint == 1:\r\n",
        "  print(a)\r\n",
        "\r\n",
        "# compute cosine similarity\r\n",
        "dot = np.dot(a, b)\r\n",
        "norm_a = np.linalg.norm(a)\r\n",
        "norm_b = np.linalg.norm(b)\r\n",
        "\r\n",
        "cos = dot / (norm_a * norm_b)\r\n",
        "\r\n",
        "aa = a.reshape(1, 512)\r\n",
        "bb = b.reshape(1, 512)\r\n",
        "cos_lib = cosine_similarity(aa, bb)\r\n",
        "cos_lib"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9998885]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3zy-yLXqwTo"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gj2Jg0WqyqL"
      },
      "source": [
        "If we go back to the sentence we used in the word embedding sub-layer, we can see that **black** and **brown** may be similar, but they are far apart:\r\n",
        "\r\n",
        "```\r\n",
        "The black cat sat on the couch and the brown dog slept on the rug.\r\n",
        "```\r\n",
        "\r\n",
        "The word **black** is in position 2, pos=2, and the word **brown** is in position 10, pos=10.\r\n",
        "\r\n",
        "Our problem is to find a way to add a value to the word embedding of each word so that it has that information. However, we need to add a value to the $d_{model} = 512$ dimensions! For each word embedding vector, we need to find a way to provide information to $i$ in the $range(0,512)$ dimensions of the word embedding vector of **black** and **brown**.\r\n",
        "\r\n",
        "There are many ways to achieve this goal. The designers found a clever way to use a unit sphere to represent positional encoding with sine and cosine values that will thus remain small but very useful.\r\n",
        "\r\n",
        "Vaswani et al. (2017) provide sine and cosine functions so that we can generate different frequencies for the positional encoding (PE) for each position and each dimension i of the dmodel = 512 of the word embedding vector:\r\n",
        "\r\n",
        "$$ PE_{(pos,2i)}=sin\\begin{pmatrix} \\frac{pos}{10000\\frac{2i}{d_{model}}} \\end{pmatrix}$$\r\n",
        "\r\n",
        "$$ PE_{(pos,2i+1)}=cos\\begin{pmatrix} \\frac{pos}{10000\\frac{2i}{d_{model}}} \\end{pmatrix}$$\r\n",
        "\r\n",
        "If we start at the beginning of the word embedding vector, we will begin with a constant (512), $i=0$, and end with $i=511$. **This means that the sine function will be applied to the even numbers and the cosine function to the odd numbers**. Some implementations do it differently. In that case, the domain of the sine function can be $i \\in [0, 255]$  and the domain of the cosine function can be $i \\in [256, 512]$. This will produce similar results.\r\n",
        "\r\n",
        "A literal translation into Python produces the following code for a positional vector $pe[0][i]$ for a position `pos`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWbYK39rg2y"
      },
      "source": [
        "def positional_encoding(pos, pe):\r\n",
        "  for i in range(0, 512, 2):\r\n",
        "    pe[0][i] = math.sin(pos / (10000 ** ((2 * i) / 512)))\r\n",
        "    pe[0][i + 1] = math.cos(pos / (10000 ** ((2 * i) / 512)))\r\n",
        "  return pe"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPhTFVxN91T-"
      },
      "source": [
        "Before going further, you might want to see the plot of the sine function, for example, for pos=2.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/pos-2-plot.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "Before going further, you might want to see the plot of the sine function, for example, for pos=10.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/pos-10-plot.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "If we go back to the sentence we are parsing in this section, we can see that black is in position pos=2 and brown is in position pos=10:\r\n",
        "\r\n",
        "```\r\n",
        "The black cat sat on the couch and the brown dog slept on the rug.\r\n",
        "```\r\n",
        "\r\n",
        "If we apply the sine and cosine functions literally for pos=2, we obtain a size=512 positional encoding vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCoRd1QN9jKw",
        "outputId": "ce5aac61-6267-4e07-fe68-34085188e76d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "d_model=512\r\n",
        "max_length=20\r\n",
        "max_len=max_length\r\n",
        "\r\n",
        "pe = torch.zeros(max_len, d_model)\r\n",
        "positional_encoding(2, pe)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9.0930e-01, -4.1615e-01,  9.5814e-01,  ...,  1.0000e+00,\n",
              "          2.1492e-08,  1.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        ...,\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O58-bxOYBXES"
      },
      "source": [
        "We also obtain a size=512, positional encoding vector for position 10, pos=10:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg2OE-yw9wN1",
        "outputId": "e285636e-6f54-43d3-b5d5-fe0c7aa5a239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "positional_encoding(10, pe)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.4402e-01, -8.3907e-01,  1.1878e-01,  ...,  1.0000e+00,\n",
              "          1.0746e-07,  1.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        ...,\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTtqNgDvCbkM"
      },
      "source": [
        "When we look at the results we obtained with an intuitive literal translation, we would now like to check whether the results are meaningful.\r\n",
        "\r\n",
        "The cosine similarity function used for word embedding comes in handy for having\r\n",
        "a better visualization of the proximity of the positions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a9W-itaBKCQ",
        "outputId": "4ac32eef-9c23-4e64-e287-d0adb20359a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cosine_similarity(positional_encoding(2, pe), positional_encoding(10, pe))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmUBhy57HhKU"
      },
      "source": [
        "The similarity between the position of the words black and brown and the lexical\r\n",
        "field (groups of words that go together) similarity is different:\r\n",
        "\r\n",
        "```python\r\n",
        "cosine_similarity(black, brown)= [[0.9998901]]\r\n",
        "```\r\n",
        "\r\n",
        "The encoding of the position shows a lower similarity value than the word\r\n",
        "embedding similarity.\r\n",
        "\r\n",
        "The positional encoding has taken these words apart. Bear in mind that word\r\n",
        "embeddings will vary with the corpus used to train them.\r\n",
        "\r\n",
        "**The problem is now how to add the positional encoding to the word embedding\r\n",
        "vectors.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sXkbymkH2dw"
      },
      "source": [
        "## Adding positional encoding to the embedding vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOw1BUCrH3YO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY9SZC-dHa7V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}