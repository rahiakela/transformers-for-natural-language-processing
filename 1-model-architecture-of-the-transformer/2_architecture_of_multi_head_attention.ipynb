{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-architecture-of-multi-head-attention.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGYprLVJN6yfEZtZy2cqaK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/transformers-for-natural-language-processing/blob/main/1-model-architecture-of-the-transformer/2_architecture_of_multi_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E79jgpLzW5Wz"
      },
      "source": [
        "## The architecture of multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvi8cwmPW6dv"
      },
      "source": [
        "**The multi-head attention sub-layer contains eight heads and is followed by postlayer normalization, which will add residual connections to the output of the sublayer and normalize it.**\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/sub-layer-1.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The input of the multi-attention sub-layer of the first layer of the encoder stack is a vector that contains the embedding and the positional encoding of each word. The next layers of the stack do not start these operations over.\r\n",
        "\r\n",
        "The dimension of the vector of each word $x_n$ of an input sequence is $d_{model} = 512$:\r\n",
        "\r\n",
        "$$\r\n",
        "pe(x_n) = [d_1=9.09297407e-01, d_2=9.09297407e-01, .., d_{512}=1.00000000e+00]\r\n",
        "$$\r\n",
        "\r\n",
        "The representation of each word $x_n$ has become a vector of $d_{model} = 512$ dimensions.\r\n",
        "\r\n",
        "**Each word is mapped to all the other words to determine how it fits in a sequence.**\r\n",
        "\r\n",
        "In the following sentence, we can see that \"it\" could be related to \"cat\" and \"rug\" in the sequence:\r\n",
        "\r\n",
        "```\r\n",
        "Sequence =The cat sat on the rug and it was dry-cleaned.\r\n",
        "```\r\n",
        "\r\n",
        "**The model will train to find out if \"it\" is related to \"cat\" or \"rug.\"** We could run a huge calculation by training the model using the $d_{model} = 512$ dimensions as they are now.\r\n",
        "\r\n",
        "However, we would only get one point of view at a time by analyzing the sequence\r\n",
        "with one $d_{model}$ block. Furthermore, it would take quite some calculation time to find other perspectives.\r\n",
        "\r\n",
        "**A better way is to divide the $d_{model} = 512$ dimensions of each word $x_n$ of $x$ (all of the words of a sequence) into $8 d_k = 64$ dimensions.**\r\n",
        "\r\n",
        "**We then can run the 8 \"heads\" in parallel to speed up the training and obtain 8 different representation subspaces of how each word relates to another:**\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/multi-head-representations.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "**You can see that there are now 8 heads running in parallel.** One head might decide that \"it\" fits well with \"cat\" and another that \"it\" fits well with \"rug\" and another that \"rug\" fits well with \"dry-cleaned.\"\r\n",
        "\r\n",
        "The output of each head is a matrix $z_i$ with a shape of $x^*d_k$ The output of a multiattention head is $Z$ defined as:\r\n",
        "\r\n",
        "$$ Z = (z_0, z_1, z_2, z_3, z_4, z_5, z_6, z_7,) $$\r\n",
        "\r\n",
        "**However, $Z$ must be concatenated so that the output of the multi-head sub-layer is not a sequence of dimensions but one lines of $xm*d_{model}$ matrix.**\r\n",
        "\r\n",
        "Before exiting the multi-head attention sub-layer, the elements of $Z$ are concatenated:\r\n",
        "\r\n",
        "$$ MultiHead(output) = Concat(z_0, z_1, z_2, z_3, z_4, z_5, z_6, z_7,) = x, d_{model} $$\r\n",
        "\r\n",
        "**Notice that each head is concatenated into $z$ that has a dimension of $d_{model} = 512$. The output of the multi-headed layer respects the constraint of the original Transformer model.**\r\n",
        "\r\n",
        "Inside each head $h_n$ of the attention mechanism, each word vector has three\r\n",
        "representations:\r\n",
        "\r\n",
        "- A query vector $(Q)$ that has a dimension of $d_q = 64$, which is activated and trained when a word vector $x_n$ seeks all of the key-value pairs of the other word vectors, including itself in self-attention\r\n",
        "- A key vector $(K)$ that has a dimension of $d_k = 64$, which will be trained to provide an attention value\r\n",
        "- A value vector $(V)$ that has a dimension of $d_v = 64$, which will be trained to provide another attention value\r\n",
        "\r\n",
        "\r\n",
        "Attention is defined as **Scaled Dot-Product Attention** which is represented in the following equation in which we plug $Q$, $K$ and $V$:\r\n",
        "\r\n",
        "$$\r\n",
        "Attention(Q,K,V) = softmax \\begin{pmatrix} \\frac{QK^T}{\\sqrt{d_k}} \\end{pmatrix} V\r\n",
        "$$\r\n",
        "\r\n",
        "**The vectors all have the same dimension making it relatively simple to use a scaled dot product to obtain the attention values for each head and then concatenate the output Z of the 8 heads.**\r\n",
        "\r\n",
        "To obtain $Q$, $K$, and $V$, we must train the model with their respective weight matrices $Q_w, K_w$ and $V_w$, which have $d_k = 64$ columns and $d_{model} = 512$ rows. For example, $Q$ is obtained by a dot-product between $x$ and $Q_w. Q$ will have a dimension of $d_k = 64$.\r\n",
        "\r\n",
        "Hugging Face and Google Brain Trax, among others, provide ready-to-use\r\n",
        "frameworks, libraries, and modules. However, let's open the hood of the Transformer model and get our hands dirty in Python to illustrate the architecture we just explored in order to visualize the model in code and show it with intermediate images.\r\n",
        "\r\n",
        "We will use basic Python code with only numpy and a softmax function in 10 steps to run the key aspects of the attention mechanism.\r\n",
        "\r\n",
        "We will start by only using minimal Python functions to understand the Transformer at a low level with the inner workings of an attention head. We will explore the inner workings of the multi-head attention sub-layer using basic code.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iVVwA51jLqZ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHMoJEBzlHFp",
        "outputId": "8673017a-5621-4a66-80d7-69e413643f3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Transformer Installation\r\n",
        "!pip -qq install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.9MB 5.6MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.2MB 26.9MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 36.2MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0d0E3GGjNez"
      },
      "source": [
        "import numpy as np\r\n",
        "from scipy.special import softmax\r\n",
        "from transformers import pipeline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeoNwYO8nGNX"
      },
      "source": [
        "## Step 1: Represent the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxx2MKwhnHSF"
      },
      "source": [
        "The input of the attention mechanism we are building is scaled down to $d_{model} = 4$ instead of $d_{model} = 512$. This brings the dimensions of the vector of an input $x$ down to $d_{model} = 4$, which is easier to visualize.\r\n",
        "\r\n",
        "x contains 3 inputs with 4 dimensions each instead of 512:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XseTj4OcmFv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ad58b0-61f1-42d3-a214-c36e22871487"
      },
      "source": [
        "print(\"Step 1: Input : 3 inputs, d_model=4\")\r\n",
        "\r\n",
        "x = np.array([\r\n",
        "    [1.0, 0.0, 1.0, 0.0],     # Input 1 \r\n",
        "    [0.0, 2.0, 0.0, 2.0],     # Input 2 \r\n",
        "    [1.0, 1.0, 1.0, 1.0],     # Input 3          \r\n",
        "])\r\n",
        "print(x.shape)  # (3x4)\r\n",
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1: Input : 3 inputs, d_model=4\n",
            "(3, 4)\n",
            "[[1. 0. 1. 0.]\n",
            " [0. 2. 0. 2.]\n",
            " [1. 1. 1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6o1dY6DmysT"
      },
      "source": [
        "The first step of our model is ready:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/multi-head-input.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ej_Q-j0ncv9"
      },
      "source": [
        "## Step 2: Initializing the weight matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcKfdDPPndnj"
      },
      "source": [
        "We will now add the weight matrices to our model.\r\n",
        "\r\n",
        "Each input has 3 weight matrices:\r\n",
        "\r\n",
        "- $Q_w$ to train the queries\r\n",
        "- $K_w$ to train the keys\r\n",
        "- $V_w$ to train the values\r\n",
        "\r\n",
        "**These 3 weight matrices will be applied to all the inputs in this model.**\r\n",
        "\r\n",
        "The weight matrices described by **Vaswani** are $d_k = 64$ dimensions.\r\n",
        "\r\n",
        "However, let's scale the matrices down to $d_k = 3$. The dimensions are scaled down to 3*4 weight matrices to be able to visualize the intermediate results more easily and perform dot products with the input $x$.\r\n",
        "\r\n",
        "The three weight matrices are initialized starting with the query weight matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1LL_v0mmqw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c72194e-96f3-4b53-980e-4018c1a36e62"
      },
      "source": [
        "print(\"Step 2: weights 3 dimensions x d_model=4\")\r\n",
        "print(\"query weight matrix\")\r\n",
        "\r\n",
        "w_query = np.array([\r\n",
        "    [1, 0, 1],\r\n",
        "    [1, 0, 0],\r\n",
        "    [0, 0, 1],\r\n",
        "    [0, 1, 1]                \r\n",
        "])\r\n",
        "print(w_query.shape)  # (4x3)\r\n",
        "print(w_query)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 2: weights 3 dimensions x d_model=4\n",
            "query weight matrix\n",
            "(4, 3)\n",
            "[[1 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyRZAGAlrZmD"
      },
      "source": [
        "We will now initialize the key weight matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv_GL8KSrBQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d9b61cc-b214-4a23-9e28-072f12b396bd"
      },
      "source": [
        "print(\"key weight matrix\")\r\n",
        "\r\n",
        "w_key = np.array([\r\n",
        "    [0, 0, 1],\r\n",
        "    [1, 1, 0],\r\n",
        "    [0, 1, 0],\r\n",
        "    [1, 1, 0]                \r\n",
        "])\r\n",
        "print(w_key.shape)  # (4x3)\r\n",
        "print(w_key)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "key weight matrix\n",
            "(4, 3)\n",
            "[[0 0 1]\n",
            " [1 1 0]\n",
            " [0 1 0]\n",
            " [1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luC13v1trtyJ"
      },
      "source": [
        "Finally, we initialize the value weight matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LQemL5FruLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e890b13b-568a-4697-8ebb-0e74d8c36a5c"
      },
      "source": [
        "print(\"value weight matrix\")\r\n",
        "\r\n",
        "w_value = np.array([\r\n",
        "    [0, 2, 0],\r\n",
        "    [0, 3, 0],\r\n",
        "    [1, 0, 3],\r\n",
        "    [1, 1, 0]                \r\n",
        "])\r\n",
        "print(w_value.shape)  # (4x3)\r\n",
        "print(w_value)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value weight matrix\n",
            "(4, 3)\n",
            "[[0 2 0]\n",
            " [0 3 0]\n",
            " [1 0 3]\n",
            " [1 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCZ8igfcsE0E"
      },
      "source": [
        "The second step of our model is ready:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/multi-head-weight-matrices.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlvNNjGesdTb"
      },
      "source": [
        "## Step 3: Matrix multiplication to obtain Q, K, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONma6Hy8seNM"
      },
      "source": [
        "We will now multiply the input vectors by the weight matrices to obtain a query,\r\n",
        "key, and value vector for each input.\r\n",
        "\r\n",
        "In this model, we will assume that there is one w_query, w_key, and w_value weight matrix for all inputs.\r\n",
        "\r\n",
        "Let's first multiply the input vectors by the w_query weight matrix:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4Tq19tzs-pn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dffa64e9-6269-45d8-f190-44d047a1f813"
      },
      "source": [
        "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\r\n",
        "\r\n",
        "print(\"Query: x * w_query\")\r\n",
        "Q = np.matmul(x, w_query)\r\n",
        "print(Q.shape)   # (3x4)(4x3) = (3x3)\r\n",
        "print(Q)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 3: Matrix multiplication to obtain Q,K,V\n",
            "Query: x * w_query\n",
            "(3, 3)\n",
            "[[1. 0. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 1. 3.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiRSHLXlthys"
      },
      "source": [
        "We now multiply the input vectors by the w_key weight matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq_hDOOdtUsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43908968-12e6-408e-b736-438c16d6fecd"
      },
      "source": [
        "print(\"Key: x * w_key\")\r\n",
        "K = np.matmul(x, w_key)\r\n",
        "print(K.shape)   # (3x4)(4x3) = (3x3)\r\n",
        "print(K)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Key: x * w_key\n",
            "(3, 3)\n",
            "[[0. 1. 1.]\n",
            " [4. 4. 0.]\n",
            " [2. 3. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZS9Wwz4t03p"
      },
      "source": [
        "Finally, we multiply the input vectors by the w_value weight matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPgwLn_nttbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62009785-7b15-41a9-f089-d7257b8ad29f"
      },
      "source": [
        "print(\"Value: x * w_value\")\r\n",
        "V = np.matmul(x, w_value)\r\n",
        "print(V.shape)   # (3x4)(4x3) = (3x3)\r\n",
        "print(V)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value: x * w_value\n",
            "(3, 3)\n",
            "[[1. 2. 3.]\n",
            " [2. 8. 0.]\n",
            " [2. 6. 3.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bPBc2aLuCLA"
      },
      "source": [
        "The third step of our model is ready:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/k-q-v.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "We have the Q, K, and V values we need to calculate the attention scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOtk5wKnvSNM"
      },
      "source": [
        "## Step 4: Scaled attention scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB8HYh5ivbM6"
      },
      "source": [
        "The attention head now implements the original Transformer equation:\r\n",
        "\r\n",
        "$$\r\n",
        "Attention(Q,K,V) = softmax \\begin{pmatrix} \\frac{QK^T}{\\sqrt{d_k}} \\end{pmatrix} V\r\n",
        "$$\r\n",
        "\r\n",
        "Step 4 focuses on Q and K:\r\n",
        "\r\n",
        "$$\r\n",
        " \\begin{pmatrix} \\frac{QK^T}{\\sqrt{d_k}} \\end{pmatrix}\r\n",
        "$$\r\n",
        "\r\n",
        "For this model, we will round $\\sqrt{ùëë_ùëò} = \\sqrt{3} = 1.75$ to 1 and plug the values into the $Q$ and $K$ part of the equation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDWH4t4Kt8ge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd670ab-e3af-41c8-8dc2-9bbc82defbef"
      },
      "source": [
        "print(\"Step 4: Scaled Attention Scores\")\r\n",
        "\r\n",
        "k_d = 1    # square root of k_d=3 rounded down to 1 for this example\r\n",
        "attention_scores = (Q @ K.transpose()) / k_d\r\n",
        "print(attention_scores)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 4: Scaled Attention Scores\n",
            "[[ 2.  4.  4.]\n",
            " [ 4. 16. 12.]\n",
            " [ 4. 12. 10.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VE2FmTIy2pr"
      },
      "source": [
        "Step 4 is now complete. For example, the score for $x_1$ is [2,4,4] across the $K$ vectors across the head as displayed:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/scaled-attention-scores.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The attention equation will now apply softmax to the intermediate scores for each vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_dvKcfAzSjj"
      },
      "source": [
        "## Step 5: Scaled softmax attention scores for each vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzocFt3LzWc3"
      },
      "source": [
        "We now apply a softmax function to each intermediate attention score. Instead of\r\n",
        "doing a matrix multiplication, let's zoom down to each individual vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo6Dgi43xvBV",
        "outputId": "7d144f00-e058-4650-abc6-8f726c545516"
      },
      "source": [
        "print(\"Step 5: Scaled softmax attention_scores for each vector\")\r\n",
        "\r\n",
        "attention_scores[0] = softmax(attention_scores[0])\r\n",
        "attention_scores[1] = softmax(attention_scores[1])\r\n",
        "attention_scores[2] = softmax(attention_scores[2])\r\n",
        "\r\n",
        "print(attention_scores[0])\r\n",
        "print(attention_scores[1])\r\n",
        "print(attention_scores[2])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 5: Scaled softmax attention_scores for each vector\n",
            "[0.06337894 0.46831053 0.46831053]\n",
            "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
            "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZBmnAeG0Sn7"
      },
      "source": [
        "Step 5 is now complete. For example, the softmax of the score of $x_1$ for all the keys is:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/softmax-score.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "We can now calculate the final attention values with the complete equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A52TC2kl0xm0"
      },
      "source": [
        "## Step 6: The final attention representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyWtBh1j02pd"
      },
      "source": [
        "We now can finalize the attention equation by plugging V in:\r\n",
        "\r\n",
        "$$\r\n",
        "Attention(Q,K,V) = softmax \\begin{pmatrix} \\frac{QK^T}{\\sqrt{d_k}} \\end{pmatrix} V\r\n",
        "$$\r\n",
        "\r\n",
        "We will first calculate the attention score of input $x_1$ for Steps 6 and 7. We calculate one attention value for one word vector. When we reach Step 8, we will generalize the attention calculation to the other two input vectors.\r\n",
        "\r\n",
        "To obtain Attention $(Q,K,V)$ for $x_1$, we multiply the intermediate attention score by the 3 value vectors one by one to zoom down into the inner workings of the equation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZDuQjWk25Ja",
        "outputId": "c0a5f795-f7d5-4d52-b79e-ec228ea02611"
      },
      "source": [
        "print(\"Step 6: attention value obtained by score1/k_d * V\")\r\n",
        "print(V[0])\r\n",
        "print(V[1])\r\n",
        "print(V[2])\r\n",
        "\r\n",
        "print(\"Attention 1\")\r\n",
        "attention1 = attention_scores[0].reshape(-1, 1)\r\n",
        "attention1 = attention_scores[0][0] * V[0]\r\n",
        "print(attention1)\r\n",
        "\r\n",
        "print(\"Attention 2\")\r\n",
        "attention2 = attention_scores[0][1] * V[1]\r\n",
        "print(attention2)\r\n",
        "\r\n",
        "print(\"Attention 3\")\r\n",
        "attention3 = attention_scores[0][2] * V[2]\r\n",
        "print(attention3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 6: attention value obtained by score1/k_d * V\n",
            "[1. 2. 3.]\n",
            "[2. 8. 0.]\n",
            "[2. 6. 3.]\n",
            "Attention 1\n",
            "[0.06337894 0.12675788 0.19013681]\n",
            "Attention 2\n",
            "[0.93662106 3.74648425 0.        ]\n",
            "Attention 3\n",
            "[0.93662106 2.80986319 1.40493159]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0da0vXq5_SO"
      },
      "source": [
        "Step 6 is complete. For example, the 3 attention values for $x_1$ for each input have been calculated:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/attention-representations.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The attention values now need to be summed up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVpi9pcw6eP2"
      },
      "source": [
        "## Step 7: Summing up the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Mghpuu6fIG"
      },
      "source": [
        "The 3 attention values of input #1 obtained will now be summed to obtain the first line of the output matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faIbsRyY3nEW",
        "outputId": "2c48eb83-b0c9-4e54-d882-51e163b4f674",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Step7: summed the results to create the first line of the output matrix\")\r\n",
        "\r\n",
        "attention_input1 = attention1 + attention2 + attention3\r\n",
        "print(attention_input1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step7: summed the results to create the first line of the output matrix\n",
            "[1.93662106 6.68310531 1.59506841]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm4R5nVuCF0U"
      },
      "source": [
        "The second line will be for the output of the next input, input #2, for example.\r\n",
        "\r\n",
        "We can see the summed attention value for $x_1$.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/summed-results.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "We have completed the steps for input #1. We now need to add the results of all the inputs to the model.\r\n",
        "\r\n",
        "The Transformer can now produce the attention values of input #2 and input #3\r\n",
        "using the same method described from Step 1 to Step 7 for one attention head."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIaiP10dCdmY"
      },
      "source": [
        "## Step 8: Steps 1 to 7 for all the inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wkiv5WUCfvT"
      },
      "source": [
        "From this step onward, we will assume we have 3 attention values with learned\r\n",
        "weights with $d_{model} = 64$. We now want to see what the original dimensions look like when they reach the sub-layer's output.\r\n",
        "\r\n",
        "We have seen the attention representation process in detail with a small model.\r\n",
        "\r\n",
        "Let's go directly to the result and assume we have generated the 3 attention\r\n",
        "representations with a dimension of  $d_{model} = 64$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0i2Nt7sBwXm",
        "outputId": "e2136415-9eff-4f51-a36f-158c62337433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Step 8: Step 1 to 7 for inputs 1 to 3\")\r\n",
        "\r\n",
        "#We assume we have 3 results with learned weights (they were not trained in this example)\r\n",
        "#We assume we are implementing the original Transformer paper.We will have 3 results of 64 dimensions each\r\n",
        "attention_head1 = np.random.random((3, 64))\r\n",
        "print(attention_head1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 8: Step 1 to 7 for inputs 1 to 3\n",
            "[[0.12960316 0.34296818 0.03969789 0.14361152 0.07498374 0.95800163\n",
            "  0.89467638 0.49062812 0.46713236 0.7728143  0.18687134 0.79028584\n",
            "  0.67430569 0.78225067 0.05925634 0.45519185 0.17252697 0.83633677\n",
            "  0.01165582 0.60787875 0.51240936 0.37324915 0.64214028 0.63820448\n",
            "  0.79299911 0.8702796  0.34480678 0.69935888 0.23876508 0.75744885\n",
            "  0.42685565 0.83580167 0.27681255 0.07192946 0.49588339 0.53287184\n",
            "  0.47960178 0.93356303 0.34215873 0.79982354 0.3318137  0.64629143\n",
            "  0.8681996  0.37812662 0.72985401 0.72515204 0.29330057 0.43922161\n",
            "  0.03884909 0.60141544 0.86516785 0.39940213 0.06315141 0.8409673\n",
            "  0.77573541 0.92034688 0.57377927 0.85792962 0.04497576 0.04073713\n",
            "  0.31409676 0.8289383  0.5800878  0.97772902]\n",
            " [0.60348128 0.20732576 0.14908406 0.52316202 0.38813038 0.63694142\n",
            "  0.6023683  0.1840806  0.29242089 0.42694083 0.70197029 0.06366125\n",
            "  0.80236701 0.64514654 0.9986585  0.97322262 0.39785542 0.08855582\n",
            "  0.55366121 0.91127003 0.0217998  0.93830918 0.89070626 0.32928705\n",
            "  0.4015822  0.12807653 0.96069962 0.92096473 0.28720477 0.61455816\n",
            "  0.04515714 0.95642298 0.5906159  0.51931551 0.59562282 0.4368149\n",
            "  0.33006118 0.16210227 0.28329761 0.45191932 0.41767641 0.00140405\n",
            "  0.95043171 0.968184   0.01541276 0.59511618 0.29388292 0.38135834\n",
            "  0.14965116 0.47891084 0.92312631 0.57096315 0.0133325  0.22759121\n",
            "  0.63160262 0.9249752  0.5640395  0.4179233  0.7246482  0.03650301\n",
            "  0.28394565 0.97904179 0.48636784 0.27187045]\n",
            " [0.53483569 0.87793443 0.70451109 0.74642148 0.6400128  0.9366772\n",
            "  0.81766505 0.91805295 0.56723223 0.85346589 0.81615966 0.36708439\n",
            "  0.65480297 0.91156076 0.68310644 0.55884657 0.35942318 0.97999572\n",
            "  0.47650219 0.98448691 0.18647285 0.04223856 0.89551789 0.61484832\n",
            "  0.41217397 0.92323097 0.72662316 0.77438994 0.77647619 0.03906452\n",
            "  0.53044009 0.97590504 0.45180098 0.76983947 0.72180381 0.95538332\n",
            "  0.28571159 0.96958183 0.75771099 0.42040281 0.20963619 0.65325587\n",
            "  0.72462302 0.34265489 0.04386336 0.68930804 0.61803596 0.11117344\n",
            "  0.78736077 0.74486965 0.62000608 0.89864474 0.17562437 0.14762114\n",
            "  0.88753645 0.18736265 0.15570647 0.64877837 0.19075182 0.32433752\n",
            "  0.61213085 0.0186421  0.17142618 0.61419232]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnt3OeE5GnNR"
      },
      "source": [
        "The above output displays the simulation of $z_0$, which represents the 3 output\r\n",
        "vectors of $d_{model} = 64$ dimensions for head 1.\r\n",
        "\r\n",
        "The Transformer now has the output vectors for the inputs of one head. The next\r\n",
        "step is to generate the outputs of the 8 heads to create the final output of the attention sub-layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjRaboPxG5kz"
      },
      "source": [
        "## Step 9: The output of the heads of the attention sub-layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea93C3gOG6RF"
      },
      "source": [
        "We assume that we have trained the 8 heads of the attention sub-layer. The\r\n",
        "transformer now has 3 output vectors (of the 3 input vectors that are words or word pieces) of $d_{model} = 64$ dimensions each:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCWAk9TaF2Yw",
        "outputId": "241083de-44ac-408c-859e-bfc5b13e9f22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Step 9: We assume we have trained the 8 heads of the attention sub-layer\")\r\n",
        "\r\n",
        "z0_h1 = np.random.random((3, 64))\r\n",
        "z1_h2 = np.random.random((3, 64))\r\n",
        "z2_h3 = np.random.random((3, 64))\r\n",
        "z3_h4 = np.random.random((3, 64))\r\n",
        "z4_h5 = np.random.random((3, 64))\r\n",
        "z5_h6 = np.random.random((3, 64))\r\n",
        "z6_h7 = np.random.random((3, 64))\r\n",
        "z7_h8 = np.random.random((3, 64))\r\n",
        "\r\n",
        "print(f\"shape of one head {z0_h1.shape} dimension of 8 heads {64 * 8}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 9: We assume we have trained the 8 heads of the attention sub-layer\n",
            "shape of one head (3, 64) dimension of 8 heads 512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DCQQs3ION_"
      },
      "source": [
        "The 8 heads have now produced $Z$:\r\n",
        "\r\n",
        "$$ Z = (z_0, z_1, z_2, z_3, z_4, z_5, z_6, z_7,) $$\r\n",
        "\r\n",
        "**The Transformer will now concatenate the 8 elements of $Z$ for the final output of the multi-head attention sub-layer.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3qiHzecIfnu"
      },
      "source": [
        "## Step 10: Concatenation of the output of the heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a26H6ta4Imib"
      },
      "source": [
        "The Transformer concatenates the 8 elements of $Z$:\r\n",
        "\r\n",
        "$$ MultiHead(output) = Concat(z_0, z_1, z_2, z_3, z_4, z_5, z_6, z_7,) W^0 = x, d_{model} $$\r\n",
        "\r\n",
        "Note that $Z$ is multiplied by $W^0$, which is a weight matrix that is trained as well.\r\n",
        "\r\n",
        "In this model, we will assume $W_0$ is trained and integrated into the concatenation function.\r\n",
        "\r\n",
        "$z_0$ to $z_7$ is concantenated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKnstbD_IFK5",
        "outputId": "51de7d1a-54e0-499e-e46c-77599b2fe6e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Step 10: Concantenation of heads 1 to 8 to obtain the original 8x64=512 ouput dimension of the model\")\r\n",
        "\r\n",
        "output_attention = np.hstack((z0_h1, z1_h2, z2_h3, z3_h4, z4_h5, z6_h7, z7_h8))\r\n",
        "print(output_attention)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 10: Concantenation of heads 1 to 8 to obtain the original 8x64=512 ouput dimension of the model\n",
            "[[0.30247523 0.21049491 0.71817656 ... 0.82521425 0.3803086  0.28336977]\n",
            " [0.43262206 0.32890745 0.29132959 ... 0.70092619 0.30360292 0.72852975]\n",
            " [0.43037306 0.94438448 0.97129937 ... 0.99269694 0.30642778 0.39679104]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQb-64rhJ_99"
      },
      "source": [
        "The concatenation can be visualized as stacking the elements of $Z$ side by side:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/sub-layer-output.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The concatenation produced a standard $d_{model} = 512$ dimensional output:\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/transformers-for-natural-language-processing/head-concatenation.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "**Layer normalization will now process the attention sub-layer.**"
      ]
    }
  ]
}